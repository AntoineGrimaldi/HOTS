%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% COSYNE-2007 Abstract Template
%%% Version 1.0
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%http://www.cosyne.org/c/index.php?title=Abstracts
%
%Submission Format
%
%Before you log onto the submission website, you should have the following items prepared: (1) title, (2) author list (including email addresses of all authors), and (3) two-page PDF submission. Submissions that do not meet the following guidelines may be rejected.
%
%Abstracts will be evaluated on the basis of a two page (A4 or US Letter) submission in PDF format. This two-page PDF should contain:
%
%    Title - 100 characters or fewer (including spaces), capitalized in sentence case
%    300-word Summary - brief description of the study's primary findings, emphasizing their significance, generality, novelty and relevance. You will be asked to copy this 300-word summary into a text-only box; it will be included in the conference program if your submission is accepted.
%    Additional Detail - use the remaining space to expand upon the central question(s), approach, results, and/or conclusions of the study. You may include equations as appropriate. Figures are optional. You need not touch upon all the major points of the Summary, but should aim to include whatever detail will best help reviewers to evaluate the significance of your study. Do not feel obliged to fill the entire two pages.
%    Double-blind - The reviewing process for Cosyne will be double blind at the level of reviewers. Authors are responsible for anonymizing their submission. In particular, do not include author names, author affiliations, or acknowledgements in the abstract or body of the submission and avoid providing any other identifying information in text or figures. If you need to cite one of your own publications, you should do so with adequate anonymization to preserve double-blind reviewing (e.g., write “In the previous work of Author et al. [1]…” rather than “In our previous work [1]...”). If you need to cite one of your own papers that is a non-anonymous preprint (on arXiv, social media, websites, etc.) please do so with adequate anonymization (e.g., if the cited submission is available as a non-anonymous preprint, then write “Author et al. [1] concurrently show…”). Reviewers will be instructed not to actively look for such preprints, but encountering them will not constitute a conflict of interest. Alternatively, authors may submit work that is already available as a preprint (e.g., on arXiv) without citing it; however, previously published papers by the authors on related topics must be cited (with adequate anonymization to preserve double-blind reviewing). 
%
%Font size (including any figure legends) must be at least 12 point. Margins should be at least 0.5". This two-page PDF will be the only document seen by reviewers. (Abstracts exceeding two pages will have additional pages removed). Submissions that do not meet these guidelines may be rejected.
%
%For questions regarding abstract submission, please contact: meeting [at] cosyne.org
%
%
%Evaluation Criteria
%
%PDF submissions will be evaluated on the basis of the following criteria:
%
%    Significance
%    Originality
%    Clarity
%    Relevance to the Cosyne audience. 
%
%The submission should clearly explain why your question is important and how your claims will advance the field. Include enough detail that reviewers can assess the technical content of your methods and results. Please be sure to address the significance and fit of your submission for the Cosyne audience, which includes a mix of experimentalists and theoreticians interested in the functional properties of neural systems. Potentially inappropriate abstracts include pure machine learning studies, or studies of single cells with no clear implications for neural systems.
%
%Approximately 30 submissions will be chosen for short talks and ~330 will be chosen for poster presentations. 
%

%\documentclass[12pt,draft]{article}
\documentclass[12pt]{article}
\usepackage{times}
%\inner 0.5in
\oddsidemargin -0.5in		% margin, in addition to 1" standard
\textwidth 7.5in		% 8.5" - 2*(1+\oddsidemargin)

\topmargin -1in		% in addition to 1.5" standard margin
\textheight 9.75in 		% 11 - ( 1.5 + \topmargin + <bottom-margin> )

\columnsep 0.25in

\parindent 0pt
\parskip 12pt

\flushbottom \sloppy
\pagestyle{empty} % No page numbers

\usepackage{subfigure}
\usepackage{tikz}
\usepackage{csquotes}
%\usepackage[footnotesize]{caption}

\usepackage[
%style=chem-acs,
style=numeric,						% numeric style for reference list
citestyle=numeric-comp,
%style=alphabetic-verb,
giveninits=false,
maxbibnames=1,
firstinits=true,
%style=apa,
%maxcitenames=1,
%maxnames=3,
%minnames=1,
%maxbibnames=99,
dateabbrev=true,
giveninits=true,
%uniquename=init,
url=false,
doi=false,
isbn=false,
eprint=false,
texencoding=utf8,
bibencoding=utf8,
autocite=superscript,
backend=biber,
%sorting=none,
sorting=nty,
sortcites=false,
%articletitle=false
]{biblatex}%
\newcommand{\citep}[1]{\parencite{#1}}
\newcommand{\citet}[1]{\textcite{#1}}
\bibliography{biblio.bib} % the ref.bib file

\usepackage{wrapfig}
\usepackage{siunitx}
%\renewcommand{\cite}{\citep}%
\newcommand{\ms}{\si{\milli\second}}%

%%%%%%%%%%% BEGIN METADATA %%%%%%%%%%%%
\newcommand{\AuthorAG}{Antoine Grimaldi}

\newcommand{\AuthorLP}{Laurent Perrinet}
\newcommand{\AuthorVB}{Victor Boutin}
\newcommand{\AddressLP}{Institut de Neurosciences de la Timone (UMR 7289); Aix Marseille Univ, CNRS; Marseille, France}%
\newcommand{\WebsiteLP}{https://laurentperrinet.github.io}%
\newcommand{\EmailLP}{Laurent.Perrinet@univ-amu.fr}%
\newcommand{\EmailRB}{xxx@xxx}%
\newcommand{\AuthorSI}{Sio-Hoi Ieng}
\newcommand{\AuthorRB}{Ryad Benosman}%
\newcommand{\orcidRB}{0000-0003-0243-944X}%
\newcommand{\AddressRB}{Sorbonne Université, INSERM, CNRS, Institut de la Vision, France;}%
\newcommand{\Summary}{ % TIP: depuis la PDF, je copie le summary et lance  "pbpaste |wc -w" -------->  285 mots
%Reverse engineering is the art of looking at an existing device in order to understand its concepts and operation. 
%We used this approach to link innovative methods used in computer vision to computational neuroscience. 
Here, we propose a neuromimetic Spiking Neural Network (SNN) architecture able to perform pattern recognition. % in an end-to-end event-based way. 
To achieve this, we extended the algorithm presented in~\citet{Lagorce17}, an event-based algorithm which introduced novel spatio-temporal features: time-surfaces. Built from asynchronous events acquired by a neuromorphic camera, these time-surfaces allow to code the local dynamics of a visual scene and to create an efficient hierarchical event-based pattern recognition architecture. 
Our work is three-fold. First, through the analysis of this existing method we were able to adapt its formalism in the computational neuroscience domain by drawing an analogy with Spiking Neural Networks of leaky integrate-and-fire models and Hebbian learning. 
Then, %after reproduction of their results, 
generalization of the classification task with a more complex and widely used dataset (N-MNIST) was performed using logistic regression. A significant contribution was to achieve online classification of the N-MNIST dataset reaching state of the art performances. To our knowledge, it is the first end-to-end event-based online classification network of this kind. The third contribution came by adding an homeostatic regulation rule inspired by biology for neural activity. According to the efficient coding hypothesis, neural activity should be equally distributed between neurons. We used this principle to force neurons in the same layer to spike on average with balanced firing rates by setting for each neuron a gain depending on their past activity. Efficiency of this technique was demonstrated through an improvement in spatio-temporal patterns which were learned during the training phase and a classification performance reaching $97,8\%$ accuracy. As a summary, %by using a method used in a different field, 
we were able to develop a neuromimetic SNN model for online digit classification. We aim at pursuing the study of this architecture through unsupervised learning of natural scene and hope to offer insights on the efficiency of the processing in neural tissues.
}


\usepackage{titlesec}
% \titlespacing*{<command>}{<left>}{<before-sep>}{<after-sep>}
\titlespacing*{\section}
{0pt}{1.5ex}{0.8ex}
\titlespacing*{\subsection}
{0pt}{0.9ex}{0.4ex}
\titlespacing*{\subsubsection}
{0pt}{0.5ex}{0.3ex}
\titlespacing*{\paragraph}{%
  0pt}{%              left margin
  0.0\baselineskip}{% space before (vertical)
  1em}%               space after (horizontal)



\begin{document}

%%%-----------------------------------------------------------------
{\Large\bf
%An end-to-end event-based image classification algorithm using a Spiking Neural Network model
%An 
Event-driven Spiking Neural Networks %model 
for
pattern recognition
%  image classification
}

%{\bf
%\AuthorAG$^{\dagger}$, \AuthorVB$^{\dagger}$, \AuthorLP$^{\dagger}$, \AuthorSI$^{\ddagger}$ and \AuthorRB$^{\ddagger}$
%}
%
%{
%$\dagger$ \AddressLP \\
%$\ddagger$ \AddressRB
%}

%%%-----------------------------------------------------------------
%%SUMMARY

\parindent 12pt

\paragraph*{Summary}
\Summary
%%END OF SUMMARY
%
%\begin{figure}[!ht]%[!ht][!b]%
\begin{wrapfigure}{R}{.5\textwidth}
\includegraphics[width=.99\linewidth]{../notebooks/fig/hots.png}
\caption
{
\textbf{The HOTS algorithm (extracted from~\citep{Lagorce17}).} The N-MNIST dataset consists of (a) digits presented to (b) the (possibly moving) event-based camera. It produces (c) ON and OFF events which are fed into Layer 1. The events are convolved with exponential kernels (d) to build event contexts from spatial receptive field %of side-length $2R1+1$, 
known as \emph{time-surfaces}. These time-surfaces are clustered into $N_1=4$ features (e). When a feature gets the smallest euclidian distance with the input, it produces an event in the channel corresponding to the matched feature (f). Events from the $N_1=4$ features constitute the output of the layer (g). Each layer takes input from its previous layer and feeds the next by reproducing steps (d)-(g). The output (i) of the last layer is then fed to the classifier (j) which will recognize the object in the input.
\label{fig:fig1}
}
\end{wrapfigure}
%\end{figure}
%: <<<<<< Laurent is here
\paragraph*{From event-based computing to Spiking Neural Networks}
% Introduction : event-based computing and engineering -> Spiking Neural Network
Blah blahblah blah blah blah. Blah blahblah blah blah blah Blah blahblah blah blah blah.
Blah blahblah blah blah blah. Blah blahblah blah blah blah Blah blahblah blah blah blah.
Blah blahblah blah blah blah. Blah blahblah blah blah blah Blah blahblah blah blah blah.
% In fact HOTS can be written as a (convolutional, hierarchical) network of IF neurons
Blah blahblah blah blah blah. Blah blahblah blah blah blah Blah blahblah blah blah blah.
Blah blahblah blah blah blah. Blah blahblah blah blah blah Blah blahblah blah blah blah.
Blah blahblah blah blah blah. Blah  blah blah blah Blah blahblah blah blah blah.
Blah blahblah blah blah blah. Blah blahblah blah blah blah Blah blahblah blah blah blah.
% and thus the learning could be seen as a form of STDP with a competition
Blah blahblah blah blah blah. Blah blahblah blah blah blah Blah blahblah blah blah blah.
Blah blahblah blah blah blah. Blah blahblah blah blah blah Blah blahblah blah blah blah.
Blah blahblah blah blah blah. Blah blahblah blah blah blah Blah blahblah blah blah blah.
% Laurent is here >>>>>>>


\paragraph*{Hebbian Learning and the role of homeostasis}
% we apply it to the N-MNIST dataset : presentation of this dataset
Blah blahblah blah blah blah. Blah blahblah blah blah blah Blah blahblah blah blah blah.
Blah blahblah blah blah blah. Blah blahblah blah blah blah Blah blahblah blah blah blah.
Blah blahblah blah blah blah. Blah blahblah blah blah blah Blah blahblah blah blah blah.
Blah blahblah blah blah blah. Blah blahblah blah blah blah Blah blahblah blah blah blah.
% we perform self-supervised learning as in Lagorce and obtain filters see Fig1 a
Blah blahblah blah blah blah. Blah blahblah blah blah blah Blah blahblah blah blah blah.
Blah blahblah blah blah blah. Blah blahblah blah blah blah Blah blahblah blah blah blah.
Blah blahblah blah blah blah. Blah blahblah blah blah blah Blah blahblah blah blah blah.
% there is an imbablance, we add homeostasis and obtain filters see Fig1 b
Blah blahblah blah blah blah. Blah blahblah blah blah blah Blah blahblah blah blah blah.
Blah blahblah blah blah blah. Blah blahblah blah blah blah Blah blahblah blah blah blah.
Blah blahblah blah blah ah blah Blah blahblah blah blah blah.
Blah blahblah blah blah blah. Blah blahblah blah blah bl blah blah blah.
%
%\begin{figure}[!ht]%[!ht][!b]%
\begin{wrapfigure}{l}{.52\textwidth}
\centering
\includegraphics[width=.95\linewidth]{layerz.png}
\caption
{Features and activation histograms obtained in the self-supervised learning algorithm. (A) and (C) represent the activation histograms for each features within the different layers of the network respectively with and without homeostasis. (B) and (D) are representations of the features learned during the training of the SNN respectively with an without homeostasis. Columns corresponds to the different polarities (ON and OFF for the first layer, output channels of the previous layer for the others) and lines are the features. In the third layer, only 10 features and 6 polarities are presented for the sake of clarity and space use.
\label{fig:fig2}
}
\end{wrapfigure}
%\end{figure}
\paragraph*{Classifying using spikes}
% HOTS uses histograms : for N-MNIST it performs quite poorly
In the original HOTS algorithm~\citet{Lagorce17}, final classification is performed by computing the histogram of activation within the last layer of the network. By comparing this histogram with that obtained on the training set, one could find the class which is closest to the current histogram. While this method performed well for original dataset (a set of digits and letters), it gives a rather poor performance of $7X.X\%$ for the N-MNIST dataset. Using the features obtained using the homeostasis did not improve the results by much, with $8X.X\%$ classification accuracy. 
% we introduce an event-based logistic / state-of-the art classification
Instead of this method we introduce a simple classification scheme based on logistic regression. Indeed, following the same strategy as for the construction of the time surfaces, each post-synaptic output of the network could integrate each output spike using a time constant that we set here to $\tau=300~\ms$. Each input spike is thus related to an output spike and therefore to an analog vector. We used this vector to introduce a model of classification which is equivalent to logistic regression~\citep{Berens12} and which is compatible with a neural representation. We found that after learning on the training subset, the network reached an accuracy of $98.X\%$ of the test subset, on par with state-of-the art algorithms.
%
\paragraph*{Perspectives}
% open-source HOTS / predictive coding / neuroscience
In this work, we have presented the implementation of a Spiking Neural Network model able to perform digit classification at a state-of-the-art performance. Its implementation is available at \url{https://github.com/XXX/YYY} and allows to reproduce all results presented here. % https://github.com/SpikeAI/HOTS
Blah blahblah blah blah blah. Blah blahblah blah blah blah Blah blahblah blah blah blah.
Blah blahblah blah blah blah. Blah blahblah blah blah blah Blah blahblah blah blah blah.
Blah blahblah blah blah blah. Blah blahblah blah blah blah Blah blahblah blah blah blah.
Blah blahblah blah blah blah. Blah blahblah blah blah blah Blah blahblah blah blah blah.
%
\paragraph*{References}
{
\small
%\footnotesize
%\begingroup
%\setstretch{0.75}
%\setlength\bibitemsep{1pt}
\printbibliography[heading=none]
%\endgroup
}


%%%-----------------------------------------------------------------
%{\bf Acknowledgments}\\
%We thank T. Colleague for helpful discussions.
%This work was supported by XXX grant xxxx.
%%%-----------------------------------------------------------------
\end{document}

