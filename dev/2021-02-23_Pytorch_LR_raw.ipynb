{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/laurentperrinet/quantic/science/HomeHots/HOTS_clone_laurent/HOTS\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%cd '../HOTS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tonic\n",
    "import numpy as np\n",
    "#from Network import *\n",
    "download = False\n",
    "learn_set = tonic.datasets.NMNIST(save_to='../Data/',\n",
    "                                  train=True, download=download,\n",
    "                                  transform=tonic.transforms.AERtoVector()\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has size 60000, using 800\n"
     ]
    }
   ],
   "source": [
    "records_path = '../Records'\n",
    "import datetime\n",
    "timestr = datetime.datetime.now().date().isoformat()\n",
    "timestr = '2021-02-13'\n",
    "verbose = True\n",
    "\n",
    "%mkdir -p ../Records\n",
    "%mkdir -p ../Records/EXP_03_NMNIST\n",
    "\n",
    "homeo = True\n",
    "sigma = None\n",
    "pooling = False\n",
    "homeinv = False\n",
    "jitter = False\n",
    "tau = 5\n",
    "krnlinit = 'first'\n",
    "nblay = 3\n",
    "nbclust = 4\n",
    "\n",
    "ds = 75\n",
    "nb_train = int(len(learn_set)//ds)\n",
    "print(f'The dataset has size {len(learn_set)}, using {nb_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def tic():\n",
    "    global ttic\n",
    "    ttic = time.time()\n",
    "def toc():\n",
    "    print(f'Done in {time.time() - ttic:.3f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "torch.set_default_tensor_type(\"torch.DoubleTensor\") # -> torch.tensor([1.2, 3]).dtype = torch.float64\n",
    "# https://sebastianraschka.com/faq/docs/pytorch-crossentropy.html\n",
    "#criterion = torch.nn.NLLLoss(reduction=\"mean\") # loss divided by output size\n",
    "criterion = torch.nn.BCELoss(reduction=\"mean\") # loss divided by output size\n",
    "\n",
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    #torch.nn.Module -> Base class for all neural network modules\n",
    "    def __init__(self, N, n_classes, bias=True):\n",
    "        super(LogisticRegressionModel, self).__init__() \n",
    "        self.linear = torch.nn.Linear(N, n_classes, bias=bias)\n",
    "        self.nl = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, factors):\n",
    "        return self.nl(self.linear(factors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "betas = (beta1, beta2)\n",
    "num_epochs = 2 ** 5 + 1\n",
    "#num_epochs = 2 ** 9 + 1\n",
    "# batch_size = 256\n",
    "n_classes=10\n",
    "amsgrad = False # gives similar results\n",
    "amsgrad = True  # gives similar results\n",
    "\n",
    "def fit_raw_data(dataset, \n",
    "            nb_digit,\n",
    "            learning_rate=learning_rate,\n",
    "            # batch_size=batch_size,  # gamma=gamma,\n",
    "            num_epochs=num_epochs,\n",
    "            betas=betas,\n",
    "            verbose=False, #**kwargs\n",
    "        ):\n",
    "    \n",
    "\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    sampler = torch.utils.data.RandomSampler(dataset, replacement=True, num_samples=nb_digit, generator=generator)\n",
    "    loader = tonic.datasets.DataLoader(dataset, sampler=sampler)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'device -> {device}')\n",
    "\n",
    "    N = 34*34*2\n",
    "    n_classes = 10\n",
    "    logistic_model = LogisticRegressionModel(N, n_classes)\n",
    "    logistic_model = logistic_model.to(device)\n",
    "    logistic_model.train()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        logistic_model.parameters(), lr=learning_rate, betas=betas, amsgrad=amsgrad\n",
    "    )\n",
    "    \n",
    "    for epoch in range(int(num_epochs)):\n",
    "        losses = []\n",
    "        for X, label in loader:\n",
    "            X, label = X.to(device), label.to(device)\n",
    "            X, label = X.squeeze(0), label.squeeze(0) # just one digit = one batch\n",
    "            outputs = logistic_model(X)\n",
    "\n",
    "            n_events = X.shape[0]\n",
    "            #print(X.squeeze(0).shape, label * torch.ones((1, n_events)))\n",
    "            #print(outputs, label)\n",
    "            labels = label*torch.ones(n_events).type(torch.LongTensor).to(device)\n",
    "            labels = torch.nn.functional.one_hot(labels, num_classes=n_classes).type(torch.DoubleTensor).to(device)\n",
    "            #print(outputs.shape, labels.shape)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        if verbose and (epoch % (num_epochs // 32) == 0):\n",
    "            print(f\"Iteration: {epoch} - Loss: {np.mean(losses):.5f}\")\n",
    "            \n",
    "    return logistic_model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device -> cpu\n",
      "Iteration: 0 - Loss: 0.30797\n",
      "Iteration: 1 - Loss: 0.27139\n",
      "Iteration: 2 - Loss: 0.24503\n",
      "Iteration: 3 - Loss: 0.25235\n",
      "Iteration: 4 - Loss: 0.27811\n",
      "Iteration: 5 - Loss: 0.30568\n",
      "Iteration: 6 - Loss: 0.22946\n",
      "Iteration: 7 - Loss: 0.25978\n",
      "Iteration: 8 - Loss: 0.25760\n",
      "Iteration: 9 - Loss: 0.23459\n",
      "Iteration: 10 - Loss: 0.29423\n",
      "Iteration: 11 - Loss: 0.28655\n",
      "Iteration: 12 - Loss: 0.24020\n",
      "Iteration: 13 - Loss: 0.27124\n",
      "Iteration: 14 - Loss: 0.26455\n",
      "Iteration: 15 - Loss: 0.22961\n",
      "Iteration: 16 - Loss: 0.22876\n",
      "Iteration: 17 - Loss: 0.23239\n",
      "Iteration: 18 - Loss: 0.25138\n",
      "Iteration: 19 - Loss: 0.21375\n",
      "Iteration: 20 - Loss: 0.21686\n",
      "Iteration: 21 - Loss: 0.25900\n",
      "Iteration: 22 - Loss: 0.20008\n",
      "Iteration: 23 - Loss: 0.23550\n",
      "Iteration: 24 - Loss: 0.21693\n",
      "Iteration: 25 - Loss: 0.30183\n",
      "Iteration: 26 - Loss: 0.25443\n",
      "Iteration: 27 - Loss: 0.25170\n",
      "Iteration: 28 - Loss: 0.23566\n",
      "Iteration: 29 - Loss: 0.25216\n",
      "Iteration: 30 - Loss: 0.23054\n",
      "Iteration: 31 - Loss: 0.21808\n",
      "Iteration: 32 - Loss: 0.25373\n",
      "Done in 2798.134 s\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "model, loss = fit_raw_data(learn_set, \n",
    "            nb_train,\n",
    "            learning_rate=learning_rate,\n",
    "            # batch_size=batch_size,  # gamma=gamma,\n",
    "            num_epochs=num_epochs,\n",
    "            betas=betas,\n",
    "            verbose=True,\n",
    "        )\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = tonic.datasets.NMNIST(save_to='../Data/',\n",
    "                                train=False, download=download,\n",
    "                                transform=tonic.transforms.AERtoVector())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has size 10000, using 133\n"
     ]
    }
   ],
   "source": [
    "nb_test = int(len(test_set)//ds)\n",
    "print(f'The dataset has size {len(test_set)}, using {nb_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(test_set, model, # gamma=gamma,\n",
    "            verbose=False, **kwargs\n",
    "        ):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "        sampler = torch.utils.data.RandomSampler(test_set, replacement=True, num_samples=nb_test, generator=generator)\n",
    "        loader = tonic.datasets.DataLoader(test_set, sampler=sampler)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        logistic_model = model.to(device)\n",
    "\n",
    "        pred_target, true_target = [], []\n",
    "\n",
    "        for X, label in loader:\n",
    "            X = X.to(device)\n",
    "            X, label = X.squeeze(0), label.squeeze(0)\n",
    "\n",
    "            n_events = X.shape[0]\n",
    "            labels = label*torch.ones(n_events).type(torch.LongTensor)\n",
    "\n",
    "            outputs = logistic_model(X)\n",
    "\n",
    "            pred_target.append(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            true_target.append(labels.numpy())\n",
    "\n",
    "    return pred_target, true_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_target, true_target = predict_data(test_set, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([1, 1, 7, ..., 7, 7, 7]),\n",
       "  array([1, 1, 1, ..., 6, 6, 6]),\n",
       "  array([1, 1, 1, ..., 6, 6, 6]),\n",
       "  array([1, 1, 1, ..., 6, 6, 6]),\n",
       "  array([7, 1, 3, ..., 0, 0, 0]),\n",
       "  array([1, 1, 1, ..., 7, 7, 7]),\n",
       "  array([7, 7, 7, ..., 8, 8, 8]),\n",
       "  array([7, 1, 1, ..., 1, 1, 1]),\n",
       "  array([4, 7, 7, ..., 8, 8, 8]),\n",
       "  array([1, 7, 7, ..., 7, 7, 7]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([7, 7, 7, ..., 9, 9, 9]),\n",
       "  array([1, 7, 7, ..., 7, 7, 7]),\n",
       "  array([1, 7, 7, ..., 3, 3, 3]),\n",
       "  array([1, 1, 7, ..., 8, 8, 8]),\n",
       "  array([1, 3, 7, ..., 7, 7, 7]),\n",
       "  array([7, 1, 1, ..., 7, 7, 7]),\n",
       "  array([7, 7, 1, ..., 8, 8, 8]),\n",
       "  array([1, 1, 1, ..., 8, 8, 8]),\n",
       "  array([1, 1, 1, ..., 9, 9, 9]),\n",
       "  array([5, 1, 1, ..., 9, 9, 9]),\n",
       "  array([1, 7, 7, ..., 8, 8, 8]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([1, 1, 1, ..., 3, 3, 3]),\n",
       "  array([7, 1, 1, ..., 3, 3, 3]),\n",
       "  array([1, 1, 1, ..., 9, 9, 9]),\n",
       "  array([7, 7, 7, ..., 4, 4, 4]),\n",
       "  array([1, 1, 1, ..., 2, 2, 2]),\n",
       "  array([1, 1, 7, ..., 6, 6, 6]),\n",
       "  array([1, 1, 7, ..., 6, 6, 6]),\n",
       "  array([7, 4, 4, ..., 9, 9, 9]),\n",
       "  array([1, 1, 1, ..., 8, 8, 8]),\n",
       "  array([1, 1, 1, ..., 2, 2, 2]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([1, 1, 1, ..., 9, 9, 9]),\n",
       "  array([1, 7, 7, ..., 9, 9, 9]),\n",
       "  array([7, 7, 7, ..., 7, 7, 7]),\n",
       "  array([1, 1, 7, ..., 9, 9, 9]),\n",
       "  array([1, 1, 1, ..., 0, 0, 0]),\n",
       "  array([7, 7, 7, ..., 8, 8, 8]),\n",
       "  array([7, 7, 7, ..., 9, 9, 9]),\n",
       "  array([7, 7, 7, ..., 1, 1, 1]),\n",
       "  array([7, 7, 7, ..., 7, 7, 7]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([7, 7, 7, ..., 6, 6, 6]),\n",
       "  array([7, 7, 7, ..., 2, 2, 2]),\n",
       "  array([1, 1, 1, ..., 3, 3, 3]),\n",
       "  array([5, 5, 5, ..., 4, 4, 4]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([7, 7, 7, ..., 8, 8, 8]),\n",
       "  array([7, 7, 7, ..., 7, 7, 7]),\n",
       "  array([1, 1, 1, ..., 3, 3, 3]),\n",
       "  array([1, 1, 7, ..., 1, 1, 1]),\n",
       "  array([7, 7, 7, ..., 2, 2, 2]),\n",
       "  array([1, 1, 5, ..., 7, 7, 7]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([1, 1, 1, ..., 6, 6, 6]),\n",
       "  array([1, 1, 0, ..., 5, 5, 5]),\n",
       "  array([7, 7, 1, ..., 8, 8, 8]),\n",
       "  array([1, 1, 1, ..., 8, 8, 8]),\n",
       "  array([7, 1, 1, ..., 9, 9, 9]),\n",
       "  array([1, 1, 1, ..., 2, 2, 2]),\n",
       "  array([1, 1, 7, ..., 9, 9, 9]),\n",
       "  array([1, 7, 1, ..., 2, 2, 2]),\n",
       "  array([1, 1, 1, ..., 2, 2, 2]),\n",
       "  array([1, 7, 1, ..., 5, 5, 5]),\n",
       "  array([1, 1, 7, ..., 7, 7, 7]),\n",
       "  array([1, 1, 1, ..., 4, 4, 4]),\n",
       "  array([4, 1, 5, ..., 0, 0, 0]),\n",
       "  array([1, 7, 1, ..., 6, 6, 6]),\n",
       "  array([1, 1, 1, ..., 6, 6, 6]),\n",
       "  array([1, 1, 1, ..., 9, 9, 9]),\n",
       "  array([1, 7, 7, ..., 8, 8, 8]),\n",
       "  array([5, 5, 1, ..., 1, 1, 1]),\n",
       "  array([1, 1, 6, ..., 8, 8, 8]),\n",
       "  array([1, 1, 1, ..., 6, 6, 6]),\n",
       "  array([1, 1, 1, ..., 8, 8, 8]),\n",
       "  array([7, 7, 7, ..., 9, 9, 9]),\n",
       "  array([1, 7, 7, ..., 0, 0, 0]),\n",
       "  array([1, 1, 1, ..., 3, 3, 3]),\n",
       "  array([1, 1, 1, ..., 8, 8, 8]),\n",
       "  array([1, 1, 4, ..., 4, 4, 4]),\n",
       "  array([1, 1, 1, ..., 6, 6, 6]),\n",
       "  array([7, 1, 1, ..., 8, 8, 8]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([1, 1, 1, ..., 2, 2, 2]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([7, 7, 7, ..., 8, 8, 8]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([1, 7, 1, ..., 2, 2, 2]),\n",
       "  array([7, 5, 7, ..., 8, 8, 8]),\n",
       "  array([1, 7, 7, ..., 0, 0, 0]),\n",
       "  array([7, 7, 7, ..., 8, 8, 8]),\n",
       "  array([1, 1, 1, ..., 2, 2, 2]),\n",
       "  array([1, 1, 1, ..., 2, 2, 2]),\n",
       "  array([7, 7, 7, ..., 4, 4, 4]),\n",
       "  array([1, 1, 7, ..., 3, 3, 3]),\n",
       "  array([1, 1, 1, ..., 9, 9, 9]),\n",
       "  array([7, 7, 7, ..., 9, 9, 9]),\n",
       "  array([7, 7, 1, ..., 8, 8, 8]),\n",
       "  array([1, 1, 5, ..., 0, 0, 0]),\n",
       "  array([1, 7, 7, ..., 4, 4, 4]),\n",
       "  array([1, 1, 7, ..., 7, 7, 7]),\n",
       "  array([7, 7, 7, ..., 8, 8, 8]),\n",
       "  array([1, 7, 7, ..., 4, 4, 4]),\n",
       "  array([1, 7, 5, ..., 8, 8, 8]),\n",
       "  array([1, 1, 1, ..., 8, 8, 8]),\n",
       "  array([1, 1, 4, ..., 9, 9, 9]),\n",
       "  array([7, 7, 7, ..., 9, 9, 9]),\n",
       "  array([7, 7, 7, ..., 1, 1, 1]),\n",
       "  array([1, 1, 1, ..., 2, 2, 2]),\n",
       "  array([1, 1, 1, ..., 9, 9, 9]),\n",
       "  array([1, 1, 1, ..., 7, 7, 7]),\n",
       "  array([7, 7, 1, ..., 6, 6, 6]),\n",
       "  array([5, 1, 1, ..., 9, 9, 9]),\n",
       "  array([1, 1, 4, ..., 2, 2, 2]),\n",
       "  array([1, 1, 1, ..., 2, 2, 2]),\n",
       "  array([1, 1, 1, ..., 8, 8, 8]),\n",
       "  array([1, 1, 1, ..., 6, 6, 6]),\n",
       "  array([1, 7, 7, ..., 2, 2, 2]),\n",
       "  array([7, 7, 7, ..., 2, 2, 2]),\n",
       "  array([1, 1, 1, ..., 0, 0, 0]),\n",
       "  array([1, 1, 1, ..., 0, 0, 0]),\n",
       "  array([7, 7, 1, ..., 7, 7, 7]),\n",
       "  array([1, 1, 1, ..., 3, 3, 3]),\n",
       "  array([7, 7, 7, ..., 7, 7, 7]),\n",
       "  array([1, 7, 5, ..., 8, 8, 4]),\n",
       "  array([1, 7, 7, ..., 2, 2, 2]),\n",
       "  array([7, 1, 1, ..., 1, 1, 1]),\n",
       "  array([7, 7, 7, ..., 9, 9, 9]),\n",
       "  array([1, 5, 5, ..., 9, 9, 9])],\n",
       " [array([7, 7, 7, ..., 7, 7, 7]),\n",
       "  array([6, 6, 6, ..., 6, 6, 6]),\n",
       "  array([6, 6, 6, ..., 6, 6, 6]),\n",
       "  array([6, 6, 6, ..., 6, 6, 6]),\n",
       "  array([0, 0, 0, ..., 0, 0, 0]),\n",
       "  array([7, 7, 7, ..., 7, 7, 7]),\n",
       "  array([8, 8, 8, ..., 8, 8, 8]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([4, 4, 4, ..., 4, 4, 4]),\n",
       "  array([7, 7, 7, ..., 7, 7, 7]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([9, 9, 9, ..., 9, 9, 9]),\n",
       "  array([7, 7, 7, ..., 7, 7, 7]),\n",
       "  array([8, 8, 8, ..., 8, 8, 8]),\n",
       "  array([8, 8, 8, ..., 8, 8, 8]),\n",
       "  array([7, 7, 7, ..., 7, 7, 7]),\n",
       "  array([9, 9, 9, ..., 9, 9, 9]),\n",
       "  array([8, 8, 8, ..., 8, 8, 8]),\n",
       "  array([8, 8, 8, ..., 8, 8, 8]),\n",
       "  array([4, 4, 4, ..., 4, 4, 4]),\n",
       "  array([9, 9, 9, ..., 9, 9, 9]),\n",
       "  array([5, 5, 5, ..., 5, 5, 5]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([3, 3, 3, ..., 3, 3, 3]),\n",
       "  array([3, 3, 3, ..., 3, 3, 3]),\n",
       "  array([9, 9, 9, ..., 9, 9, 9]),\n",
       "  array([4, 4, 4, ..., 4, 4, 4]),\n",
       "  array([2, 2, 2, ..., 2, 2, 2]),\n",
       "  array([6, 6, 6, ..., 6, 6, 6]),\n",
       "  array([6, 6, 6, ..., 6, 6, 6]),\n",
       "  array([2, 2, 2, ..., 2, 2, 2]),\n",
       "  array([0, 0, 0, ..., 0, 0, 0]),\n",
       "  array([0, 0, 0, ..., 0, 0, 0]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([9, 9, 9, ..., 9, 9, 9]),\n",
       "  array([9, 9, 9, ..., 9, 9, 9]),\n",
       "  array([2, 2, 2, ..., 2, 2, 2]),\n",
       "  array([9, 9, 9, ..., 9, 9, 9]),\n",
       "  array([0, 0, 0, ..., 0, 0, 0]),\n",
       "  array([8, 8, 8, ..., 8, 8, 8]),\n",
       "  array([9, 9, 9, ..., 9, 9, 9]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([7, 7, 7, ..., 7, 7, 7]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([8, 8, 8, ..., 8, 8, 8]),\n",
       "  array([2, 2, 2, ..., 2, 2, 2]),\n",
       "  array([5, 5, 5, ..., 5, 5, 5]),\n",
       "  array([5, 5, 5, ..., 5, 5, 5]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([4, 4, 4, ..., 4, 4, 4]),\n",
       "  array([7, 7, 7, ..., 7, 7, 7]),\n",
       "  array([2, 2, 2, ..., 2, 2, 2]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([0, 0, 0, ..., 0, 0, 0]),\n",
       "  array([4, 4, 4, ..., 4, 4, 4]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([2, 2, 2, ..., 2, 2, 2]),\n",
       "  array([5, 5, 5, ..., 5, 5, 5]),\n",
       "  array([5, 5, 5, ..., 5, 5, 5]),\n",
       "  array([8, 8, 8, ..., 8, 8, 8]),\n",
       "  array([5, 5, 5, ..., 5, 5, 5]),\n",
       "  array([9, 9, 9, ..., 9, 9, 9]),\n",
       "  array([4, 4, 4, ..., 4, 4, 4]),\n",
       "  array([2, 2, 2, ..., 2, 2, 2]),\n",
       "  array([2, 2, 2, ..., 2, 2, 2]),\n",
       "  array([5, 5, 5, ..., 5, 5, 5]),\n",
       "  array([7, 7, 7, ..., 7, 7, 7]),\n",
       "  array([4, 4, 4, ..., 4, 4, 4]),\n",
       "  array([0, 0, 0, ..., 0, 0, 0]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([6, 6, 6, ..., 6, 6, 6]),\n",
       "  array([4, 4, 4, ..., 4, 4, 4]),\n",
       "  array([8, 8, 8, ..., 8, 8, 8]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([8, 8, 8, ..., 8, 8, 8]),\n",
       "  array([6, 6, 6, ..., 6, 6, 6]),\n",
       "  array([0, 0, 0, ..., 0, 0, 0]),\n",
       "  array([4, 4, 4, ..., 4, 4, 4]),\n",
       "  array([0, 0, 0, ..., 0, 0, 0]),\n",
       "  array([5, 5, 5, ..., 5, 5, 5]),\n",
       "  array([8, 8, 8, ..., 8, 8, 8]),\n",
       "  array([4, 4, 4, ..., 4, 4, 4]),\n",
       "  array([7, 7, 7, ..., 7, 7, 7]),\n",
       "  array([5, 5, 5, ..., 5, 5, 5]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([2, 2, 2, ..., 2, 2, 2]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([8, 8, 8, ..., 8, 8, 8]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([2, 2, 2, ..., 2, 2, 2]),\n",
       "  array([8, 8, 8, ..., 8, 8, 8]),\n",
       "  array([0, 0, 0, ..., 0, 0, 0]),\n",
       "  array([5, 5, 5, ..., 5, 5, 5]),\n",
       "  array([2, 2, 2, ..., 2, 2, 2]),\n",
       "  array([2, 2, 2, ..., 2, 2, 2]),\n",
       "  array([4, 4, 4, ..., 4, 4, 4]),\n",
       "  array([3, 3, 3, ..., 3, 3, 3]),\n",
       "  array([9, 9, 9, ..., 9, 9, 9]),\n",
       "  array([4, 4, 4, ..., 4, 4, 4]),\n",
       "  array([5, 5, 5, ..., 5, 5, 5]),\n",
       "  array([0, 0, 0, ..., 0, 0, 0]),\n",
       "  array([9, 9, 9, ..., 9, 9, 9]),\n",
       "  array([7, 7, 7, ..., 7, 7, 7]),\n",
       "  array([7, 7, 7, ..., 7, 7, 7]),\n",
       "  array([4, 4, 4, ..., 4, 4, 4]),\n",
       "  array([9, 9, 9, ..., 9, 9, 9]),\n",
       "  array([0, 0, 0, ..., 0, 0, 0]),\n",
       "  array([9, 9, 9, ..., 9, 9, 9]),\n",
       "  array([9, 9, 9, ..., 9, 9, 9]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([2, 2, 2, ..., 2, 2, 2]),\n",
       "  array([8, 8, 8, ..., 8, 8, 8]),\n",
       "  array([9, 9, 9, ..., 9, 9, 9]),\n",
       "  array([6, 6, 6, ..., 6, 6, 6]),\n",
       "  array([9, 9, 9, ..., 9, 9, 9]),\n",
       "  array([2, 2, 2, ..., 2, 2, 2]),\n",
       "  array([2, 2, 2, ..., 2, 2, 2]),\n",
       "  array([5, 5, 5, ..., 5, 5, 5]),\n",
       "  array([6, 6, 6, ..., 6, 6, 6]),\n",
       "  array([5, 5, 5, ..., 5, 5, 5]),\n",
       "  array([5, 5, 5, ..., 5, 5, 5]),\n",
       "  array([0, 0, 0, ..., 0, 0, 0]),\n",
       "  array([0, 0, 0, ..., 0, 0, 0]),\n",
       "  array([7, 7, 7, ..., 7, 7, 7]),\n",
       "  array([3, 3, 3, ..., 3, 3, 3]),\n",
       "  array([9, 9, 9, ..., 9, 9, 9]),\n",
       "  array([4, 4, 4, ..., 4, 4, 4]),\n",
       "  array([2, 2, 2, ..., 2, 2, 2]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([7, 7, 7, ..., 7, 7, 7]),\n",
       "  array([4, 4, 4, ..., 4, 4, 4])])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_target, true_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3808 3808\n",
      "3808\n",
      "0.9994747899159664\n",
      "4645 4645\n",
      "4645\n",
      "0.9745963401506996\n",
      "3595 3595\n",
      "3595\n",
      "0.9874826147426982\n",
      "4304 4304\n",
      "4304\n",
      "0.9769981412639405\n",
      "5272 5272\n",
      "5272\n",
      "0.9850151745068285\n",
      "4337 4337\n",
      "4337\n",
      "0.8033202674659903\n",
      "4947 4947\n",
      "4947\n",
      "0.9777643015969274\n",
      "1598 1598\n",
      "1598\n",
      "0.9993742177722152\n",
      "3659 3659\n",
      "3659\n",
      "0.12462421426619295\n",
      "3479 3479\n",
      "3479\n",
      "0.997413049726933\n",
      "2483 2483\n",
      "2483\n",
      "1.0\n",
      "4235 4235\n",
      "4235\n",
      "0.9943329397874853\n",
      "3379 3379\n",
      "3379\n",
      "0.9997040544539805\n",
      "4791 4791\n",
      "4791\n",
      "0.9169275725318305\n",
      "5527 5527\n",
      "5527\n",
      "0.988420481273747\n",
      "4186 4186\n",
      "4186\n",
      "0.9995222169135213\n",
      "4492 4492\n",
      "4492\n",
      "0.9508014247551202\n",
      "4006 4006\n",
      "4006\n",
      "0.9952571143285073\n",
      "5346 5346\n",
      "5346\n",
      "0.98989898989899\n",
      "3102 3102\n",
      "3102\n",
      "0.8175370728562218\n",
      "4481 4481\n",
      "4481\n",
      "0.9064940861414863\n",
      "4491 4491\n",
      "4491\n",
      "0.15185927410376307\n",
      "2734 2734\n",
      "2734\n",
      "0.9842721287490855\n",
      "2880 2880\n",
      "2880\n",
      "1.0\n",
      "6213 6213\n",
      "6213\n",
      "0.9233864477708031\n",
      "4953 4953\n",
      "4953\n",
      "0.9917221885725823\n",
      "3396 3396\n",
      "3396\n",
      "0.9151943462897526\n",
      "3452 3452\n",
      "3452\n",
      "0.9979721900347625\n",
      "4508 4508\n",
      "4508\n",
      "0.9944543034605147\n",
      "4508 4508\n",
      "4508\n",
      "0.9966725820763088\n",
      "5550 5550\n",
      "5550\n",
      "0.9931531531531531\n",
      "1974 1974\n",
      "1974\n",
      "0.08915906788247213\n",
      "5290 5290\n",
      "5290\n",
      "0.9703213610586011\n",
      "6741 6741\n",
      "6741\n",
      "0.8497255600059338\n",
      "2772 2772\n",
      "2772\n",
      "1.0\n",
      "3582 3582\n",
      "3582\n",
      "0.9916247906197655\n",
      "3770 3770\n",
      "3770\n",
      "0.9702917771883289\n",
      "4094 4094\n",
      "4094\n",
      "0.0\n",
      "3744 3744\n",
      "3744\n",
      "0.5200320512820513\n",
      "4204 4204\n",
      "4204\n",
      "0.9935775451950524\n",
      "4443 4443\n",
      "4443\n",
      "0.991672293495386\n",
      "4436 4436\n",
      "4436\n",
      "0.9941388638412985\n",
      "2559 2559\n",
      "2559\n",
      "0.9355216881594373\n",
      "2756 2756\n",
      "2756\n",
      "1.0\n",
      "1747 1747\n",
      "1747\n",
      "0.9954207212364052\n",
      "4197 4197\n",
      "4197\n",
      "0.3135573028353586\n",
      "4262 4262\n",
      "4262\n",
      "0.9633974659784139\n",
      "4338 4338\n",
      "4338\n",
      "0.5078377132319041\n",
      "4336 4336\n",
      "4336\n",
      "0.17366236162361623\n",
      "2164 2164\n",
      "2164\n",
      "1.0\n",
      "2972 2972\n",
      "2972\n",
      "0.24562584118438763\n",
      "3433 3433\n",
      "3433\n",
      "1.0\n",
      "5909 5909\n",
      "5909\n",
      "0.8321204941614486\n",
      "2889 2889\n",
      "2889\n",
      "0.9993077189338871\n",
      "3118 3118\n",
      "3118\n",
      "0.721616420782553\n",
      "3825 3825\n",
      "3825\n",
      "0.5552941176470588\n",
      "3410 3410\n",
      "3410\n",
      "0.9780058651026393\n",
      "2773 2773\n",
      "2773\n",
      "1.0\n",
      "3087 3087\n",
      "3087\n",
      "0.9627470035633301\n",
      "4893 4893\n",
      "4893\n",
      "0.3789086450030656\n",
      "6908 6908\n",
      "6908\n",
      "0.05920671685002895\n",
      "4649 4649\n",
      "4649\n",
      "0.99311679931168\n",
      "3149 3149\n",
      "3149\n",
      "0.12988250238170848\n",
      "5432 5432\n",
      "5432\n",
      "0.9688880706921944\n",
      "3878 3878\n",
      "3878\n",
      "0.7552862300154719\n",
      "5488 5488\n",
      "5488\n",
      "0.9954446064139941\n",
      "4295 4295\n",
      "4295\n",
      "0.9594877764842841\n",
      "2094 2094\n",
      "2094\n",
      "0.06208213944603629\n",
      "3170 3170\n",
      "3170\n",
      "0.795583596214511\n",
      "3448 3448\n",
      "3448\n",
      "0.9759280742459396\n",
      "5252 5252\n",
      "5252\n",
      "0.9982863670982483\n",
      "2624 2624\n",
      "2624\n",
      "0.5807926829268293\n",
      "3932 3932\n",
      "3932\n",
      "0.9984740590030519\n",
      "3262 3262\n",
      "3262\n",
      "0.9267320662170447\n",
      "5091 5091\n",
      "5091\n",
      "0.4926340601060695\n",
      "2073 2073\n",
      "2073\n",
      "0.9975880366618427\n",
      "2988 2988\n",
      "2988\n",
      "0.7372824631860776\n",
      "4402 4402\n",
      "4402\n",
      "0.9963652885052249\n",
      "5459 5459\n",
      "5459\n",
      "0.6056054222385052\n",
      "3288 3288\n",
      "3288\n",
      "0.11222627737226278\n",
      "5241 5241\n",
      "5241\n",
      "0.9887426063728296\n",
      "4243 4243\n",
      "4243\n",
      "0.6292717416921989\n",
      "5096 5096\n",
      "5096\n",
      "0.9937205651491365\n",
      "4838 4838\n",
      "4838\n",
      "0.9989665150888797\n",
      "1859 1859\n",
      "1859\n",
      "0.2345346960731576\n",
      "4533 4533\n",
      "4533\n",
      "0.5100375027575557\n",
      "2610 2610\n",
      "2610\n",
      "1.0\n",
      "4490 4490\n",
      "4490\n",
      "0.9848552338530067\n",
      "1656 1656\n",
      "1656\n",
      "1.0\n",
      "4055 4055\n",
      "4055\n",
      "0.7499383477188656\n",
      "3354 3354\n",
      "3354\n",
      "0.9997018485390579\n",
      "4357 4357\n",
      "4357\n",
      "0.9685563461097085\n",
      "4442 4442\n",
      "4442\n",
      "0.9837910850968032\n",
      "6489 6489\n",
      "6489\n",
      "0.9090768993681615\n",
      "3364 3364\n",
      "3364\n",
      "0.47443519619500596\n",
      "2710 2710\n",
      "2710\n",
      "0.9896678966789668\n",
      "5746 5746\n",
      "5746\n",
      "0.9942568743473721\n",
      "4099 4099\n",
      "4099\n",
      "0.9826787021224689\n",
      "4979 4979\n",
      "4979\n",
      "0.9971881903996787\n",
      "3582 3582\n",
      "3582\n",
      "0.9916247906197655\n",
      "4697 4697\n",
      "4697\n",
      "0.3176495635512029\n",
      "4756 4756\n",
      "4756\n",
      "0.27207737594617326\n",
      "5491 5491\n",
      "5491\n",
      "0.9360772172646148\n",
      "4579 4579\n",
      "4579\n",
      "0.5094998908058528\n",
      "3756 3756\n",
      "3756\n",
      "0.9994675186368477\n",
      "4277 4277\n",
      "4277\n",
      "0.8082768295534253\n",
      "3172 3172\n",
      "3172\n",
      "0.9911727616645649\n",
      "6706 6706\n",
      "6706\n",
      "0.0\n",
      "4824 4824\n",
      "4824\n",
      "0.9210199004975125\n",
      "4638 4638\n",
      "4638\n",
      "0.9956877964639931\n",
      "4148 4148\n",
      "4148\n",
      "0.9770973963355835\n",
      "2602 2602\n",
      "2602\n",
      "0.9773251345119139\n",
      "5063 5063\n",
      "5063\n",
      "0.8447560734742248\n",
      "3148 3148\n",
      "3148\n",
      "0.9825285895806861\n",
      "3496 3496\n",
      "3496\n",
      "0.6390160183066361\n",
      "6212 6212\n",
      "6212\n",
      "0.3668705730843529\n",
      "4048 4048\n",
      "4048\n",
      "0.9965415019762845\n",
      "4003 4003\n",
      "4003\n",
      "0.9972520609542843\n",
      "3836 3836\n",
      "3836\n",
      "0.9833159541188738\n",
      "3782 3782\n",
      "3782\n",
      "0.0671602326811211\n",
      "3507 3507\n",
      "3507\n",
      "0.9957228400342173\n",
      "5258 5258\n",
      "5258\n",
      "0.45435526816279953\n",
      "6548 6548\n",
      "6548\n",
      "0.8349114233353696\n",
      "6535 6535\n",
      "6535\n",
      "0.9776587605202754\n",
      "5115 5115\n",
      "5115\n",
      "0.9843597262952102\n",
      "3591 3591\n",
      "3591\n",
      "0.9986076301865775\n",
      "6215 6215\n",
      "6215\n",
      "0.3275945293644409\n",
      "5033 5033\n",
      "5033\n",
      "0.9539042320683488\n",
      "3759 3759\n",
      "3759\n",
      "0.9906890130353817\n",
      "5494 5494\n",
      "5494\n",
      "0.8325445941026575\n",
      "2244 2244\n",
      "2244\n",
      "0.9688057040998217\n",
      "3756 3756\n",
      "3756\n",
      "0.380457933972311\n",
      "3300 3300\n",
      "3300\n",
      "0.9457575757575758\n"
     ]
    }
   ],
   "source": [
    "for pred_target_, true_target_ in zip(pred_target, true_target):\n",
    "    print(len(pred_target_), len(true_target_))\n",
    "    print(len(pred_target_ == true_target_))\n",
    "    print(np.mean((pred_target_ == true_target_)*1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.mean(accuracy)=0.8046414454320989\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "for pred_target_, true_target_ in zip(pred_target, true_target):\n",
    "    accuracy.append(np.mean(pred_target_ == true_target_))\n",
    "print(f'{np.mean(accuracy)=}')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
