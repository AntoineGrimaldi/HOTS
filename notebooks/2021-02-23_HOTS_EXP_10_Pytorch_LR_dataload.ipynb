{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/laurent/quantic/science/HomeHots/HOTS_clone_laurent/HOTS\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%cd '../HOTS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../Data/nmnist_train.zip\n",
      "Extracting ../Data/nmnist_train.zip to ../Data/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "from mix_Network import *\n",
    "download = False\n",
    "learn_set = tonic.datasets.NMNIST(save_to='../Data/',\n",
    "                                  train=True, download=download,\n",
    "                                  transform=tonic.transforms.AERtoVector()\n",
    "                                 )\n",
    "records_path = '../Records'\n",
    "timestr = datetime.datetime.now().date().isoformat()\n",
    "timestr = '2021-02-13'\n",
    "verbose = True\n",
    "\n",
    "%mkdir -p ../Records\n",
    "%mkdir -p ../Records/EXP_03_NMNIST\n",
    "\n",
    "homeo = True\n",
    "sigma = None\n",
    "pooling = False\n",
    "homeinv = False\n",
    "jitter = False\n",
    "tau = 5\n",
    "krnlinit = 'first'\n",
    "nblay = 3\n",
    "nbclust = 4\n",
    "\n",
    "ds = 3/2\n",
    "ds = 75\n",
    "nb_train = int(7500//ds)\n",
    "nb_test = int(2500//ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def tic():\n",
    "    global ttic\n",
    "    ttic = time.time()\n",
    "def toc():\n",
    "    print(f'Done in {time.time() - ttic:.3f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "torch.set_default_tensor_type(\"torch.DoubleTensor\") # -> torch.tensor([1.2, 3]).dtype = torch.float64\n",
    "# https://sebastianraschka.com/faq/docs/pytorch-crossentropy.html\n",
    "#criterion = torch.nn.NLLLoss(reduction=\"mean\") # loss divided by output size\n",
    "criterion = torch.nn.BCELoss(reduction=\"mean\") # loss divided by output size\n",
    "\n",
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    #torch.nn.Module -> Base class for all neural network modules\n",
    "    def __init__(self, N, n_classes, bias=True):\n",
    "        super(LogisticRegressionModel, self).__init__() \n",
    "        self.linear = torch.nn.Linear(N, n_classes, bias=bias)\n",
    "        self.nl = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, factors):\n",
    "        return self.nl(self.linear(factors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "betas = (beta1, beta2)\n",
    "num_epochs = 2 ** 5 + 1\n",
    "#num_epochs = 2 ** 9 + 1\n",
    "# batch_size = 256\n",
    "n_classes=10\n",
    "amsgrad = False # gives similar results\n",
    "amsgrad = True  # gives similar results\n",
    "\n",
    "def fit_raw_data(dataset, \n",
    "            nb_digit,\n",
    "            learning_rate=learning_rate,\n",
    "            # batch_size=batch_size,  # gamma=gamma,\n",
    "            num_epochs=num_epochs,\n",
    "            betas=betas,\n",
    "            verbose=False, #**kwargs\n",
    "        ):\n",
    "    \n",
    "\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    sampler = torch.utils.data.RandomSampler(dataset, replacement=True, num_samples=nb_digit, generator=generator)\n",
    "    loader = tonic.datasets.DataLoader(dataset, sampler=sampler)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'device -> {device}')\n",
    "\n",
    "    N = 34*34*2\n",
    "    n_classes = 10\n",
    "    logistic_model = LogisticRegressionModel(N, n_classes)\n",
    "    logistic_model = logistic_model.to(device)\n",
    "    logistic_model.train()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        logistic_model.parameters(), lr=learning_rate, betas=betas, amsgrad=amsgrad\n",
    "    )\n",
    "    \n",
    "    for epoch in range(int(num_epochs)):\n",
    "        losses = []\n",
    "        for X, label in loader:\n",
    "            X, label = X.to(device), label.to(device)\n",
    "            X, label = X.squeeze(0), label.squeeze(0) # just one digit = one batch\n",
    "            outputs = logistic_model(X)\n",
    "\n",
    "            n_events = X.shape[0]\n",
    "            #print(X.squeeze(0).shape, label * torch.ones((1, n_events)))\n",
    "            #print(outputs, label)\n",
    "            labels = label*torch.ones(n_events).type(torch.LongTensor).to(device)\n",
    "            labels = torch.nn.functional.one_hot(labels, num_classes=n_classes).type(torch.DoubleTensor).to(device)\n",
    "            #print(outputs.shape, labels.shape)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        if verbose and (epoch % (num_epochs // 32) == 0):\n",
    "            print(f\"Iteration: {epoch} - Loss: {np.mean(losses):.5f}\")\n",
    "            \n",
    "    return logistic_model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device -> cuda\n",
      "Iteration: 0 - Loss: 0.42352\n",
      "Iteration: 1 - Loss: 0.41369\n",
      "Iteration: 2 - Loss: 0.26805\n",
      "Iteration: 3 - Loss: 0.32691\n",
      "Iteration: 4 - Loss: 0.23339\n",
      "Iteration: 5 - Loss: 0.21581\n",
      "Iteration: 6 - Loss: 0.25537\n",
      "Iteration: 7 - Loss: 0.27334\n",
      "Iteration: 8 - Loss: 0.18722\n",
      "Iteration: 9 - Loss: 0.25079\n",
      "Iteration: 10 - Loss: 0.31585\n",
      "Iteration: 11 - Loss: 0.27489\n",
      "Iteration: 12 - Loss: 0.25002\n",
      "Iteration: 13 - Loss: 0.25090\n",
      "Iteration: 14 - Loss: 0.28778\n",
      "Iteration: 15 - Loss: 0.30497\n",
      "Iteration: 16 - Loss: 0.19089\n",
      "Iteration: 17 - Loss: 0.31607\n",
      "Iteration: 18 - Loss: 0.32918\n",
      "Iteration: 19 - Loss: 0.26271\n",
      "Iteration: 20 - Loss: 0.28166\n",
      "Iteration: 21 - Loss: 0.20387\n",
      "Iteration: 22 - Loss: 0.16766\n",
      "Iteration: 23 - Loss: 0.25229\n",
      "Iteration: 24 - Loss: 0.29053\n",
      "Iteration: 25 - Loss: 0.30063\n",
      "Iteration: 26 - Loss: 0.27631\n",
      "Iteration: 27 - Loss: 0.21539\n",
      "Iteration: 28 - Loss: 0.24402\n",
      "Iteration: 29 - Loss: 0.34751\n",
      "Iteration: 30 - Loss: 0.21999\n",
      "Iteration: 31 - Loss: 0.22052\n",
      "Iteration: 32 - Loss: 0.25277\n",
      "Done in 472.737 s\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "model, loss = fit_raw_data(learn_set, \n",
    "            nb_train,\n",
    "            learning_rate=learning_rate,\n",
    "            # batch_size=batch_size,  # gamma=gamma,\n",
    "            num_epochs=num_epochs,\n",
    "            betas=betas,\n",
    "            verbose=True,\n",
    "        )\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../Data/nmnist_test.zip\n",
      "Extracting ../Data/nmnist_test.zip to ../Data/\n"
     ]
    }
   ],
   "source": [
    "test_set = tonic.datasets.NMNIST(save_to='../Data/',\n",
    "                                train=False, download=download,\n",
    "                                transform=tonic.transforms.AERtoVector())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(test_set, model, # gamma=gamma,\n",
    "            verbose=False, **kwargs\n",
    "        ):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "        sampler = torch.utils.data.RandomSampler(test_set, replacement=True, num_samples=nb_test, generator=generator)\n",
    "        loader = tonic.datasets.DataLoader(test_set, sampler=sampler)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        logistic_model = model.to(device)\n",
    "\n",
    "        pred_target, true_target = [], []\n",
    "\n",
    "        for X, label in loader:\n",
    "            X = X.to(device)\n",
    "            X, label = X.squeeze(0), label.squeeze(0)\n",
    "\n",
    "            n_events = X.shape[0]\n",
    "            labels = label*torch.ones(n_events).type(torch.LongTensor)\n",
    "\n",
    "            outputs = logistic_model(X)\n",
    "\n",
    "            pred_target.append(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            true_target.append(labels.numpy())\n",
    "\n",
    "    return pred_target, true_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_target, true_target = predict_data(test_set, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([1, 1, 1, ..., 7, 7, 7]),\n",
       "  array([1, 1, 1, ..., 6, 6, 6]),\n",
       "  array([1, 1, 1, ..., 6, 6, 6]),\n",
       "  array([1, 1, 1, ..., 6, 6, 6]),\n",
       "  array([1, 1, 4, ..., 0, 0, 0]),\n",
       "  array([1, 1, 1, ..., 3, 3, 3]),\n",
       "  array([1, 1, 1, ..., 3, 3, 3]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([1, 1, 1, ..., 8, 8, 8]),\n",
       "  array([1, 7, 7, ..., 7, 7, 7]),\n",
       "  array([1, 1, 1, ..., 3, 3, 3]),\n",
       "  array([1, 1, 1, ..., 9, 9, 9]),\n",
       "  array([1, 7, 1, ..., 7, 7, 7]),\n",
       "  array([1, 7, 7, ..., 3, 3, 3]),\n",
       "  array([1, 7, 7, ..., 3, 3, 3]),\n",
       "  array([1, 1, 3, ..., 7, 7, 7]),\n",
       "  array([1, 1, 1, ..., 7, 7, 7]),\n",
       "  array([1, 1, 1, ..., 3, 3, 3]),\n",
       "  array([1, 1, 1, ..., 8, 8, 8]),\n",
       "  array([1, 4, 1, ..., 8, 8, 8]),\n",
       "  array([1, 1, 1, ..., 9, 9, 9]),\n",
       "  array([1, 1, 1, ..., 8, 8, 8]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([1, 1, 7, ..., 3, 3, 3]),\n",
       "  array([1, 1, 1, ..., 3, 3, 3]),\n",
       "  array([1, 1, 1, ..., 9, 9, 9]),\n",
       "  array([1, 5, 0, ..., 4, 4, 4]),\n",
       "  array([1, 1, 0, ..., 2, 2, 2]),\n",
       "  array([1, 1, 7, ..., 6, 6, 6]),\n",
       "  array([1, 7, 7, ..., 6, 6, 6]),\n",
       "  array([1, 1, 1, ..., 4, 4, 4]),\n",
       "  array([1, 1, 1, ..., 0, 0, 0])],\n",
       " [array([7, 7, 7, ..., 7, 7, 7]),\n",
       "  array([6, 6, 6, ..., 6, 6, 6]),\n",
       "  array([6, 6, 6, ..., 6, 6, 6]),\n",
       "  array([6, 6, 6, ..., 6, 6, 6]),\n",
       "  array([0, 0, 0, ..., 0, 0, 0]),\n",
       "  array([7, 7, 7, ..., 7, 7, 7]),\n",
       "  array([8, 8, 8, ..., 8, 8, 8]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([4, 4, 4, ..., 4, 4, 4]),\n",
       "  array([7, 7, 7, ..., 7, 7, 7]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([9, 9, 9, ..., 9, 9, 9]),\n",
       "  array([7, 7, 7, ..., 7, 7, 7]),\n",
       "  array([8, 8, 8, ..., 8, 8, 8]),\n",
       "  array([8, 8, 8, ..., 8, 8, 8]),\n",
       "  array([7, 7, 7, ..., 7, 7, 7]),\n",
       "  array([9, 9, 9, ..., 9, 9, 9]),\n",
       "  array([8, 8, 8, ..., 8, 8, 8]),\n",
       "  array([8, 8, 8, ..., 8, 8, 8]),\n",
       "  array([4, 4, 4, ..., 4, 4, 4]),\n",
       "  array([9, 9, 9, ..., 9, 9, 9]),\n",
       "  array([5, 5, 5, ..., 5, 5, 5]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([1, 1, 1, ..., 1, 1, 1]),\n",
       "  array([3, 3, 3, ..., 3, 3, 3]),\n",
       "  array([3, 3, 3, ..., 3, 3, 3]),\n",
       "  array([9, 9, 9, ..., 9, 9, 9]),\n",
       "  array([4, 4, 4, ..., 4, 4, 4]),\n",
       "  array([2, 2, 2, ..., 2, 2, 2]),\n",
       "  array([6, 6, 6, ..., 6, 6, 6]),\n",
       "  array([6, 6, 6, ..., 6, 6, 6]),\n",
       "  array([2, 2, 2, ..., 2, 2, 2]),\n",
       "  array([0, 0, 0, ..., 0, 0, 0])])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_target, true_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3808 3808\n",
      "3808\n",
      "0.8022584033613446\n",
      "4645 4645\n",
      "4645\n",
      "0.9898815931108719\n",
      "3595 3595\n",
      "3595\n",
      "0.9955493741307372\n",
      "4304 4304\n",
      "4304\n",
      "0.9818773234200744\n",
      "5272 5272\n",
      "5272\n",
      "0.9779969650986343\n",
      "4337 4337\n",
      "4337\n",
      "0.301129813234955\n",
      "4947 4947\n",
      "4947\n",
      "0.5476046088538509\n",
      "1598 1598\n",
      "1598\n",
      "1.0\n",
      "3659 3659\n",
      "3659\n",
      "0.5649084449303088\n",
      "3479 3479\n",
      "3479\n",
      "0.9833285426846795\n",
      "2483 2483\n",
      "2483\n",
      "0.873942811115586\n",
      "4235 4235\n",
      "4235\n",
      "0.9506493506493506\n",
      "3379 3379\n",
      "3379\n",
      "0.9949689257176679\n",
      "4791 4791\n",
      "4791\n",
      "0.0002087246921310791\n",
      "5527 5527\n",
      "5527\n",
      "0.9714130631445631\n",
      "4186 4186\n",
      "4186\n",
      "0.9581939799331104\n",
      "4492 4492\n",
      "4492\n",
      "0.9165182546749777\n",
      "4006 4006\n",
      "4006\n",
      "0.654018971542686\n",
      "5346 5346\n",
      "5346\n",
      "0.9784885895997008\n",
      "3102 3102\n",
      "3102\n",
      "0.6963249516441006\n",
      "4481 4481\n",
      "4481\n",
      "0.9848248158893104\n",
      "4491 4491\n",
      "4491\n",
      "0.7252282342462704\n",
      "2734 2734\n",
      "2734\n",
      "0.9959765910753475\n",
      "2880 2880\n",
      "2880\n",
      "1.0\n",
      "6213 6213\n",
      "6213\n",
      "0.9930790278448415\n",
      "4953 4953\n",
      "4953\n",
      "0.9852614577024026\n",
      "3396 3396\n",
      "3396\n",
      "0.9967608951707891\n",
      "3452 3452\n",
      "3452\n",
      "0.9727694090382387\n",
      "4508 4508\n",
      "4508\n",
      "0.9924578527062999\n",
      "4508 4508\n",
      "4508\n",
      "0.9966725820763088\n",
      "5550 5550\n",
      "5550\n",
      "0.9963963963963964\n",
      "1974 1974\n",
      "1974\n",
      "0.028875379939209727\n",
      "5290 5290\n",
      "5290\n",
      "0.9939508506616257\n"
     ]
    }
   ],
   "source": [
    "for pred_target_, true_target_ in zip(pred_target, true_target):\n",
    "    print(len(pred_target_), len(true_target_))\n",
    "    print(len(pred_target_ == true_target_))\n",
    "    print(np.mean((pred_target_ == true_target_)*1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.mean(accuracy)=0.8424701874026173\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "for pred_target_, true_target_ in zip(pred_target, true_target):\n",
    "    accuracy.append(np.mean(pred_target_ == true_target_))\n",
    "print(f'{np.mean(accuracy)=}')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
