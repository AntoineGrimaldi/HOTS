{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/laurentperrinet/quantic/science/HomeHots/HOTS_clone_laurent/HOTS\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%cd '../HOTS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "from mix_Network import *\n",
    "\n",
    "dataset = 'nmnist'\n",
    "records_path = '../Records'\n",
    "timestr = datetime.datetime.now().date().isoformat()\n",
    "timestr = '2021-02-13'\n",
    "verbose = True\n",
    "\n",
    "%mkdir -p ../Records\n",
    "%mkdir -p ../Records/EXP_03_NMNIST\n",
    "\n",
    "homeo = True\n",
    "sigma = None\n",
    "pooling = False\n",
    "homeinv = False\n",
    "jitter = False\n",
    "tau = 5\n",
    "krnlinit = 'first'\n",
    "nblay = 3\n",
    "nbclust = 4\n",
    "\n",
    "ds = 3/2\n",
    "ds = 750\n",
    "nb_train = int(7500//ds)\n",
    "nb_test = int(2500//ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def tic():\n",
    "    global ttic\n",
    "    ttic = time.time()\n",
    "def toc():\n",
    "    print(f'Done in {time.time() - ttic:.3f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "torch.set_default_tensor_type(\"torch.DoubleTensor\") # -> torch.tensor([1.2, 3]).dtype = torch.float64\n",
    "criterion = torch.nn.NLLLoss(reduction=\"mean\") # loss divided by output size\n",
    "# https://sebastianraschka.com/faq/docs/pytorch-crossentropy.html\n",
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    #torch.nn.Module -> Base class for all neural network modules\n",
    "    def __init__(self, N, n_classes, bias=True):\n",
    "        super(LogisticRegressionModel, self).__init__() \n",
    "        self.linear = torch.nn.Linear(N, n_classes, bias=bias)\n",
    "        self.nl = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, factors):\n",
    "        return self.nl(self.linear(factors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "betas = (beta1, beta2)\n",
    "num_epochs = 2 ** 9 + 1\n",
    "batch_size = 256\n",
    "n_classes=10\n",
    "amsgrad = False # gives similar results\n",
    "amsgrad = True  # gives similar results\n",
    "\n",
    "def fit_raw_data(dataset, \n",
    "            nb_digit,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,  # gamma=gamma,\n",
    "            num_epochs=num_epochs,\n",
    "            betas=betas,\n",
    "            verbose=False, #**kwargs\n",
    "        ):\n",
    "    \n",
    "    learningset = tonic.datasets.NMNIST(save_to='../Data/',\n",
    "                                        train=True,\n",
    "                                        transform=tonic.transforms.AERtoVector())\n",
    "    loader = tonic.datasets.DataLoader(learningset, shuffle=True)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'device -> {device}')\n",
    "\n",
    "    N = 34*34*2\n",
    "    n_classes = 10\n",
    "    logistic_model = LogisticRegressionModel(N, n_classes)\n",
    "    logistic_model = logistic_model.to(device)\n",
    "    logistic_model.train()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        logistic_model.parameters(), lr=learning_rate, betas=betas, amsgrad=amsgrad\n",
    "    )\n",
    "    \n",
    "    for epoch in range(int(num_epochs)):\n",
    "        losses = []\n",
    "        for i in range(nb_digit):\n",
    "            X_, labels_ = next(iter(loader))\n",
    "            X_, labels_ = X_.to(device), labels_.to(device)\n",
    "            outputs = logistic_model(X_)\n",
    "            #target = torch.argmax(torch.squeeze(labels_,dim=1), 1)\n",
    "            loss = criterion(outputs, torch.nn.functional.one_hot(labels_, num_classes=n_classes))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        if verbose and (epoch % (num_epochs // 32) == 0):\n",
    "            print(f\"Iteration: {epoch} - Loss: {np.mean(losses):.5f}\")\n",
    "            \n",
    "    return logistic_model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../Data/nmnist_train.zip\n",
      "Extracting ../Data/nmnist_train.zip to ../Data/\n",
      "device -> cpu\n",
      "Iteration: 0 - Loss: 5.30066\n",
      "Iteration: 16 - Loss: 2.13704\n",
      "Iteration: 32 - Loss: 1.66638\n",
      "Iteration: 48 - Loss: 1.56062\n",
      "Iteration: 64 - Loss: 1.51190\n",
      "Iteration: 80 - Loss: 1.23469\n",
      "Iteration: 96 - Loss: 1.16081\n",
      "Iteration: 112 - Loss: 1.15595\n",
      "Iteration: 128 - Loss: 1.13485\n",
      "Iteration: 144 - Loss: 1.05592\n",
      "Iteration: 160 - Loss: 1.03762\n",
      "Iteration: 176 - Loss: 1.06497\n",
      "Iteration: 192 - Loss: 0.98060\n",
      "Iteration: 208 - Loss: 0.97420\n",
      "Iteration: 224 - Loss: 0.87093\n",
      "Iteration: 240 - Loss: 0.89340\n",
      "Iteration: 256 - Loss: 0.85311\n",
      "Iteration: 272 - Loss: 0.70053\n",
      "Iteration: 288 - Loss: 0.75226\n",
      "Iteration: 304 - Loss: 0.81549\n",
      "Iteration: 320 - Loss: 0.78877\n",
      "Iteration: 336 - Loss: 0.80841\n",
      "Iteration: 352 - Loss: 0.77050\n",
      "Iteration: 368 - Loss: 0.73178\n",
      "Iteration: 384 - Loss: 0.69867\n",
      "Iteration: 400 - Loss: 0.74289\n",
      "Iteration: 416 - Loss: 0.64376\n",
      "Iteration: 432 - Loss: 0.72699\n",
      "Iteration: 448 - Loss: 0.71557\n",
      "Iteration: 464 - Loss: 0.73684\n",
      "Iteration: 480 - Loss: 0.61503\n",
      "Iteration: 496 - Loss: 0.64278\n",
      "Iteration: 512 - Loss: 0.55015\n",
      "Done in 602.435 s\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "model, loss = fit_raw_data(dataset, \n",
    "            nb_train,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,  # gamma=gamma,\n",
    "            num_epochs=num_epochs,\n",
    "            betas=betas,\n",
    "            verbose=True,\n",
    "        )\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "f_name = f'{records_path}/{dataset}/models/{timestr}_LRmodel_raw.pkl'\n",
    "with open(f_name, 'wb') as file:\n",
    "    pickle.dump([model], file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mtonic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mT_co\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSampler\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_sampler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSampler\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_workers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcollate_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdrop_last\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mworker_init_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmultiprocessing_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprefetch_factor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpersistent_workers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Data loader. Combines a dataset and a sampler, and provides an iterable over\n",
       "the given dataset.\n",
       "\n",
       "The :class:`~torch.utils.data.DataLoader` supports both map-style and\n",
       "iterable-style datasets with single- or multi-process loading, customizing\n",
       "loading order and optional automatic batching (collation) and memory pinning.\n",
       "\n",
       "See :py:mod:`torch.utils.data` documentation page for more details.\n",
       "\n",
       "Arguments:\n",
       "    dataset (Dataset): dataset from which to load the data.\n",
       "    batch_size (int, optional): how many samples per batch to load\n",
       "        (default: ``1``).\n",
       "    shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
       "        at every epoch (default: ``False``).\n",
       "    sampler (Sampler or Iterable, optional): defines the strategy to draw\n",
       "        samples from the dataset. Can be any ``Iterable`` with ``__len__``\n",
       "        implemented. If specified, :attr:`shuffle` must not be specified.\n",
       "    batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\n",
       "        returns a batch of indices at a time. Mutually exclusive with\n",
       "        :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\n",
       "        and :attr:`drop_last`.\n",
       "    num_workers (int, optional): how many subprocesses to use for data\n",
       "        loading. ``0`` means that the data will be loaded in the main process.\n",
       "        (default: ``0``)\n",
       "    collate_fn (callable, optional): merges a list of samples to form a\n",
       "        mini-batch of Tensor(s).  Used when using batched loading from a\n",
       "        map-style dataset.\n",
       "    pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
       "        into CUDA pinned memory before returning them.  If your data elements\n",
       "        are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
       "        see the example below.\n",
       "    drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
       "        if the dataset size is not divisible by the batch size. If ``False`` and\n",
       "        the size of dataset is not divisible by the batch size, then the last batch\n",
       "        will be smaller. (default: ``False``)\n",
       "    timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
       "        from workers. Should always be non-negative. (default: ``0``)\n",
       "    worker_init_fn (callable, optional): If not ``None``, this will be called on each\n",
       "        worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
       "        input, after seeding and before data loading. (default: ``None``)\n",
       "    prefetch_factor (int, optional, keyword-only arg): Number of sample loaded\n",
       "        in advance by each worker. ``2`` means there will be a total of\n",
       "        2 * num_workers samples prefetched across all workers. (default: ``2``)\n",
       "    persistent_workers (bool, optional): If ``True``, the data loader will not shutdown\n",
       "        the worker processes after a dataset has been consumed once. This allows to \n",
       "        maintain the workers `Dataset` instances alive. (default: ``False``)\n",
       "\n",
       "\n",
       ".. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n",
       "             cannot be an unpicklable object, e.g., a lambda function. See\n",
       "             :ref:`multiprocessing-best-practices` on more details related\n",
       "             to multiprocessing in PyTorch.\n",
       "\n",
       ".. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n",
       "             When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n",
       "             it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\n",
       "             rounding depending on :attr:`drop_last`, regardless of multi-process loading\n",
       "             configurations. This represents the best guess PyTorch can make because PyTorch\n",
       "             trusts user :attr:`dataset` code in correctly handling multi-process\n",
       "             loading to avoid duplicate data.\n",
       "\n",
       "             However, if sharding results in multiple workers having incomplete last batches,\n",
       "             this estimate can still be inaccurate, because (1) an otherwise complete batch can\n",
       "             be broken into multiple ones and (2) more than one batch worth of samples can be\n",
       "             dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\n",
       "             cases in general.\n",
       "\n",
       "             See `Dataset Types`_ for more details on these two types of datasets and how\n",
       "             :class:`~torch.utils.data.IterableDataset` interacts with\n",
       "             `Multi-process data loading`_.\n",
       "\u001b[0;31mFile:\u001b[0m           /usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tonic.datasets.DataLoader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../Data/nmnist_test.zip\n",
      "Extracting ../Data/nmnist_test.zip to ../Data/\n"
     ]
    }
   ],
   "source": [
    "learningset = tonic.datasets.NMNIST(save_to='../Data/',\n",
    "                                train=False,\n",
    "                                transform=tonic.transforms.AERtoVector())\n",
    "\n",
    "generator=torch.Generator().manual_seed(42)\n",
    "sampler = torch.utils.data.RandomSampler(full_dataset, replacement=True, num_samples=nb_test, generator=generator)\n",
    "loader = tonic.datasets.DataLoader(learningset, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.sampler.num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(model, # gamma=gamma,\n",
    "            verbose=False, **kwargs\n",
    "        ):\n",
    "    \n",
    "    learningset = tonic.datasets.NMNIST(save_to='../Data/',\n",
    "                                    train=False,\n",
    "                                    transform=tonic.transforms.AERtoVector())\n",
    "    loader = tonic.datasets.DataLoader(learningset, shuffle=True)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    logistic_model = model.to(device)\n",
    "    \n",
    "    pred_target = []\n",
    "    \n",
    "    X, label = next(iter(loader))\n",
    "    X = X.to(device)\n",
    "    \n",
    "    outputs = logistic_model(X)\n",
    "\n",
    "    pred_target.append(torch.argmax(outputs, dim=1).item())\n",
    "\n",
    "    return pred_target, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../Data/nmnist_test.zip\n",
      "Extracting ../Data/nmnist_test.zip to ../Data/\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pred_target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2a3f82540be6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-648018503aa8>\u001b[0m in \u001b[0;36mpredict_data\u001b[0;34m(model, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mpred_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpred_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_target' is not defined"
     ]
    }
   ],
   "source": [
    "pred, output = predict_data(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing tonic functional aer_to_vect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tonic\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "learningset = tonic.datasets.NMNIST(save_to='../Data/',\n",
    "                                    train=False,\n",
    "                                    transform=tonic.transforms.AERtoVector())\n",
    "loader = tonic.datasets.DataLoader(learningset, shuffle=True)\n",
    "    \n",
    "events, target = next(iter(loader))\n",
    "print(f'digit -> {target.item()}')\n",
    "plt.imshow(torch.squeeze(events, dim=0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le moment c'est trop long, l'idée était de regrouper différents digits, de faire un shuffle pour que le fit ne se fasse pas sur tout les events d'un digit, puis tous les events de l'autre etc... C'est l'idée du batch mais du coup ça fait des gros batch (environ 4000 x nb_digit au lieu de 256). Je sais pas encore comment faire de petits batchs avec différents digits tout en utilisant le dataloader.  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learning_rate = 0.005\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "betas = (beta1, beta2)\n",
    "num_epochs = 2 ** 9 + 1\n",
    "batch_size = 32\n",
    "n_classes=10\n",
    "amsgrad = False # gives similar results\n",
    "amsgrad = True  # gives similar results\n",
    "\n",
    "def fit_raw_data(dataset, \n",
    "            nb_digit,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,  # gamma=gamma,\n",
    "            num_epochs=num_epochs,\n",
    "            betas=betas,\n",
    "            verbose=False, #**kwargs\n",
    "        ):\n",
    "    \n",
    "    learningset = tonic.datasets.NMNIST(save_to='../Data/',\n",
    "                                    train=False,\n",
    "                                    transform=tonic.transforms.AERtoVector())\n",
    "    loader = tonic.datasets.DataLoader(learningset, shuffle=True)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'device -> {device}')\n",
    "\n",
    "    N = 34*34*2\n",
    "    n_classes = 10\n",
    "    logistic_model = LogisticRegressionModel(N, n_classes)\n",
    "    logistic_model = logistic_model.to(device)\n",
    "    logistic_model.train()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        logistic_model.parameters(), lr=learning_rate, betas=betas, amsgrad=amsgrad\n",
    "    )\n",
    "    \n",
    "    for epoch in range(int(num_epochs)):\n",
    "        losses = []\n",
    "        for n_batch in range(nb_digit//batch_size):\n",
    "            X_ = torch.zeros(1,1,N)\n",
    "            labels_ = torch.tensor([])\n",
    "            for n_dig in range(batch_size):\n",
    "                x, l = next(iter(loader))\n",
    "                X_ = torch.cat([X_,x],dim=1)\n",
    "                l = torch.ones([x.shape[1]])*l\n",
    "                labels_ = torch.cat([labels_,l])\n",
    "            X_ = X_[:,1:,:]\n",
    "            X_, labels_ = X_.to(device), labels_.to(device)\n",
    "            rid = torch.randperm(X_.shape[1])\n",
    "            X_ = X_[:,rid,:]\n",
    "            labels_ = labels_[rid]\n",
    "            outputs = logistic_model(X_)\n",
    "            #target = torch.argmax(torch.squeeze(labels_,dim=1), 1)\n",
    "            #loss = criterion(torch.squeeze(outputs), torch.nn.functional.one_hot(labels_.long(), num_classes=n_classes))\n",
    "            loss = criterion(torch.squeeze(outputs), labels_.long())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        if verbose and (epoch % (num_epochs // 32) == 0):\n",
    "            print(f\"Iteration: {epoch} - Loss: {np.mean(losses):.5f}\")\n",
    "            \n",
    "    return logistic_model, losses"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tic()\n",
    "model, loss = fit_raw_data(dataset, \n",
    "            nb_train,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=32,  # gamma=gamma,\n",
    "            num_epochs=num_epochs,\n",
    "            betas=betas,\n",
    "            verbose=True,\n",
    "        )\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name = 'lala.pkl'\n",
    "f_name[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
