{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/home/antoine/homhots/HOTS/HOTS\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%cd '../HOTS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "from mix_Network import *\n",
    "\n",
    "dataset = 'nmnist'\n",
    "records_path = '../Records'\n",
    "timestr = datetime.datetime.now().date().isoformat()\n",
    "timestr = '2021-02-13'\n",
    "verbose = True\n",
    "\n",
    "%mkdir -p ../Records\n",
    "%mkdir -p ../Records/EXP_03_NMNIST\n",
    "\n",
    "homeo = True\n",
    "sigma = None\n",
    "pooling = False\n",
    "homeinv = False\n",
    "jitter = False\n",
    "tau = 5\n",
    "krnlinit = 'first'\n",
    "nblay = 3\n",
    "nbclust = 4\n",
    "\n",
    "ds = 3/2\n",
    "ds = 750\n",
    "nb_train = int(7500//ds)\n",
    "nb_test = int(2500//ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def tic():\n",
    "    global ttic\n",
    "    ttic = time.time()\n",
    "def toc():\n",
    "    print(f'Done in {time.time() - ttic:.3f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "torch.set_default_tensor_type(\"torch.DoubleTensor\") # -> torch.tensor([1.2, 3]).dtype = torch.float64\n",
    "criterion = torch.nn.NLLLoss(reduction=\"mean\") # loss divided by output size\n",
    "# https://sebastianraschka.com/faq/docs/pytorch-crossentropy.html\n",
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    #torch.nn.Module -> Base class for all neural network modules\n",
    "    def __init__(self, N, n_classes, bias=True):\n",
    "        super(LogisticRegressionModel, self).__init__() \n",
    "        self.linear = torch.nn.Linear(N, n_classes, bias=bias)\n",
    "        self.nl = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, factors):\n",
    "        return self.nl(self.linear(factors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "betas = (beta1, beta2)\n",
    "num_epochs = 2 ** 9 + 1\n",
    "batch_size = 256\n",
    "n_classes=10\n",
    "amsgrad = False # gives similar results\n",
    "amsgrad = True  # gives similar results\n",
    "\n",
    "def fit_raw_data(dataset, \n",
    "            nb_digit,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,  # gamma=gamma,\n",
    "            num_epochs=num_epochs,\n",
    "            betas=betas,\n",
    "            verbose=False, #**kwargs\n",
    "        ):\n",
    "    \n",
    "    learningset = tonic.datasets.NMNIST(save_to='../Data/',\n",
    "                                    train=True,\n",
    "                                    transform=tonic.transforms.AERtoVector())\n",
    "    loader = tonic.datasets.DataLoader(learningset, shuffle=True)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'device -> {device}')\n",
    "\n",
    "    N = 34*34*2\n",
    "    n_classes = 10\n",
    "    logistic_model = LogisticRegressionModel(N, n_classes)\n",
    "    logistic_model = logistic_model.to(device)\n",
    "    logistic_model.train()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        logistic_model.parameters(), lr=learning_rate, betas=betas, amsgrad=amsgrad\n",
    "    )\n",
    "    \n",
    "    for epoch in range(int(num_epochs)):\n",
    "        losses = []\n",
    "        for i in range(nb_digit):\n",
    "            X_, labels_ = next(iter(loader))\n",
    "            X_, labels_ = X_.to(device), labels_.to(device)\n",
    "            outputs = logistic_model(X_)\n",
    "            #target = torch.argmax(torch.squeeze(labels_,dim=1), 1)\n",
    "            loss = criterion(outputs, torch.nn.functional.one_hot(labels_, num_classes=n_classes))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        if verbose and (epoch % (num_epochs // 32) == 0):\n",
    "            print(f\"Iteration: {epoch} - Loss: {np.mean(losses):.5f}\")\n",
    "            \n",
    "    return logistic_model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../Data/nmnist_train.zip\n",
      "Extracting ../Data/nmnist_train.zip to ../Data/\n",
      "device -> cuda\n",
      "Iteration: 0 - Loss: 5.12822\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "model, loss = fit_raw_data(dataset, \n",
    "            nb_train,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,  # gamma=gamma,\n",
    "            num_epochs=num_epochs,\n",
    "            betas=betas,\n",
    "            verbose=True,\n",
    "        )\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "f_name = f'{records_path}/{dataset}/models/{timestr}_LRmodel_raw.pkl'\n",
    "with open(f_name, 'wb') as file:\n",
    "    pickle.dump([model], file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(model, # gamma=gamma,\n",
    "            verbose=False, **kwargs\n",
    "        ):\n",
    "    \n",
    "    learningset = tonic.datasets.NMNIST(save_to='../Data/',\n",
    "                                    train=False,\n",
    "                                    transform=tonic.transforms.AERtoVector())\n",
    "    loader = tonic.datasets.DataLoader(learningset, shuffle=True)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    X, label = next(iter(loader))\n",
    "    logistic_model = model.to(device)\n",
    "    X = X.to(device)\n",
    "    \n",
    "    outputs = logistic_model(X)\n",
    "\n",
    "    pred_target.append(torch.argmax(outputs, dim=1).item())\n",
    "\n",
    "    return pred_target, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, output = predict_data(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing tonic functional aer_to_vect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tonic\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "learningset = tonic.datasets.NMNIST(save_to='../Data/',\n",
    "                                    train=False,\n",
    "                                    transform=tonic.transforms.AERtoVector())\n",
    "loader = tonic.datasets.DataLoader(learningset, shuffle=True)\n",
    "    \n",
    "events, target = next(iter(loader))\n",
    "print(f'digit -> {target.item()}')\n",
    "plt.imshow(torch.squeeze(events, dim=0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le moment c'est trop long, l'idée était de regrouper différents digits, de faire un shuffle pour que le fit ne se fasse pas sur tout les events d'un digit, puis tous les events de l'autre etc... C'est l'idée du batch mais du coup ça fait des gros batch (environ 4000 x nb_digit au lieu de 256). Je sais pas encore comment faire de petits batchs avec différents digits tout en utilisant le dataloader.  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learning_rate = 0.005\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "betas = (beta1, beta2)\n",
    "num_epochs = 2 ** 9 + 1\n",
    "batch_size = 32\n",
    "n_classes=10\n",
    "amsgrad = False # gives similar results\n",
    "amsgrad = True  # gives similar results\n",
    "\n",
    "def fit_raw_data(dataset, \n",
    "            nb_digit,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,  # gamma=gamma,\n",
    "            num_epochs=num_epochs,\n",
    "            betas=betas,\n",
    "            verbose=False, #**kwargs\n",
    "        ):\n",
    "    \n",
    "    learningset = tonic.datasets.NMNIST(save_to='../Data/',\n",
    "                                    train=False,\n",
    "                                    transform=tonic.transforms.AERtoVector())\n",
    "    loader = tonic.datasets.DataLoader(learningset, shuffle=True)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'device -> {device}')\n",
    "\n",
    "    N = 34*34*2\n",
    "    n_classes = 10\n",
    "    logistic_model = LogisticRegressionModel(N, n_classes)\n",
    "    logistic_model = logistic_model.to(device)\n",
    "    logistic_model.train()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        logistic_model.parameters(), lr=learning_rate, betas=betas, amsgrad=amsgrad\n",
    "    )\n",
    "    \n",
    "    for epoch in range(int(num_epochs)):\n",
    "        losses = []\n",
    "        for n_batch in range(nb_digit//batch_size):\n",
    "            X_ = torch.zeros(1,1,N)\n",
    "            labels_ = torch.tensor([])\n",
    "            for n_dig in range(batch_size):\n",
    "                x, l = next(iter(loader))\n",
    "                X_ = torch.cat([X_,x],dim=1)\n",
    "                l = torch.ones([x.shape[1]])*l\n",
    "                labels_ = torch.cat([labels_,l])\n",
    "            X_ = X_[:,1:,:]\n",
    "            X_, labels_ = X_.to(device), labels_.to(device)\n",
    "            rid = torch.randperm(X_.shape[1])\n",
    "            X_ = X_[:,rid,:]\n",
    "            labels_ = labels_[rid]\n",
    "            outputs = logistic_model(X_)\n",
    "            #target = torch.argmax(torch.squeeze(labels_,dim=1), 1)\n",
    "            #loss = criterion(torch.squeeze(outputs), torch.nn.functional.one_hot(labels_.long(), num_classes=n_classes))\n",
    "            loss = criterion(torch.squeeze(outputs), labels_.long())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        if verbose and (epoch % (num_epochs // 32) == 0):\n",
    "            print(f\"Iteration: {epoch} - Loss: {np.mean(losses):.5f}\")\n",
    "            \n",
    "    return logistic_model, losses"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tic()\n",
    "model, loss = fit_raw_data(dataset, \n",
    "            nb_train,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=32,  # gamma=gamma,\n",
    "            num_epochs=num_epochs,\n",
    "            betas=betas,\n",
    "            verbose=True,\n",
    "        )\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lala'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_name = 'lala.pkl'\n",
    "f_name[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
