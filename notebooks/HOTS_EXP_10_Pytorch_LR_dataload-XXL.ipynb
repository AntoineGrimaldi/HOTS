{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/laurent/quantic/science/HomeHots/HOTS_clone_laurent/HOTS\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%cd '../HOTS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "from mix_Network import *\n",
    "download = False\n",
    "learn_set = tonic.datasets.NMNIST(save_to='../Data/',\n",
    "                                  train=True, download=download,\n",
    "                                  transform=tonic.transforms.AERtoVector()\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "records_path = '../Records'\n",
    "timestr = datetime.datetime.now().date().isoformat()\n",
    "timestr = '2021-02-13'\n",
    "verbose = True\n",
    "\n",
    "%mkdir -p ../Records\n",
    "%mkdir -p ../Records/EXP_03_NMNIST\n",
    "\n",
    "homeo = True\n",
    "sigma = None\n",
    "pooling = False\n",
    "homeinv = False\n",
    "jitter = False\n",
    "tau = 5\n",
    "krnlinit = 'first'\n",
    "nblay = 3\n",
    "nbclust = 4\n",
    "\n",
    "ds = 5\n",
    "nb_train = int(len(learn_set)//ds)\n",
    "print(f'The dataset has size {len(learn_set)}, using {nb_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def tic():\n",
    "    global ttic\n",
    "    ttic = time.time()\n",
    "def toc():\n",
    "    print(f'Done in {time.time() - ttic:.3f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "torch.set_default_tensor_type(\"torch.DoubleTensor\") # -> torch.tensor([1.2, 3]).dtype = torch.float64\n",
    "# https://sebastianraschka.com/faq/docs/pytorch-crossentropy.html\n",
    "#criterion = torch.nn.NLLLoss(reduction=\"mean\") # loss divided by output size\n",
    "criterion = torch.nn.BCELoss(reduction=\"mean\") # loss divided by output size\n",
    "\n",
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    #torch.nn.Module -> Base class for all neural network modules\n",
    "    def __init__(self, N, n_classes, bias=True):\n",
    "        super(LogisticRegressionModel, self).__init__() \n",
    "        self.linear = torch.nn.Linear(N, n_classes, bias=bias)\n",
    "        self.nl = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, factors):\n",
    "        return self.nl(self.linear(factors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.005\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "betas = (beta1, beta2)\n",
    "num_epochs = 2 ** 5 + 1\n",
    "#num_epochs = 2 ** 9 + 1\n",
    "num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 256\n",
    "n_classes=10\n",
    "amsgrad = False # gives similar results\n",
    "amsgrad = True  # gives similar results\n",
    "\n",
    "def fit_raw_data(dataset, \n",
    "            nb_digit,\n",
    "            learning_rate=learning_rate,\n",
    "            # batch_size=batch_size,  # gamma=gamma,\n",
    "            num_epochs=num_epochs,\n",
    "            betas=betas,\n",
    "            verbose=False, #**kwargs\n",
    "        ):\n",
    "    \n",
    "\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    sampler = torch.utils.data.RandomSampler(dataset, replacement=True, num_samples=nb_digit, generator=generator)\n",
    "    loader = tonic.datasets.DataLoader(dataset, sampler=sampler)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'device -> {device}')\n",
    "\n",
    "    N = 34*34*2\n",
    "    n_classes = 10\n",
    "    logistic_model = LogisticRegressionModel(N, n_classes)\n",
    "    logistic_model = logistic_model.to(device)\n",
    "    logistic_model.train()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        logistic_model.parameters(), lr=learning_rate, betas=betas, amsgrad=amsgrad\n",
    "    )\n",
    "    \n",
    "    for epoch in range(int(num_epochs)):\n",
    "        losses = []\n",
    "        for X, label in loader:\n",
    "            X, label = X.to(device), label.to(device)\n",
    "            X, label = X.squeeze(0), label.squeeze(0) # just one digit = one batch\n",
    "            outputs = logistic_model(X)\n",
    "\n",
    "            n_events = X.shape[0]\n",
    "            #print(X.squeeze(0).shape, label * torch.ones((1, n_events)))\n",
    "            #print(outputs, label)\n",
    "            labels = label*torch.ones(n_events).type(torch.LongTensor).to(device)\n",
    "            labels = torch.nn.functional.one_hot(labels, num_classes=n_classes).type(torch.DoubleTensor).to(device)\n",
    "            #print(outputs.shape, labels.shape)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        if verbose and (epoch % (num_epochs // 32) == 0):\n",
    "            print(f\"Iteration: {epoch} - Loss: {np.mean(losses):.5f}\")\n",
    "            \n",
    "    return logistic_model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device -> cuda\n",
      "Iteration: 0 - Loss: 0.28580\n",
      "Iteration: 1 - Loss: 0.26015\n",
      "Iteration: 2 - Loss: 0.28539\n",
      "Iteration: 3 - Loss: 0.24377\n",
      "Iteration: 4 - Loss: 0.26148\n",
      "Iteration: 5 - Loss: 0.28811\n",
      "Iteration: 6 - Loss: 0.25134\n",
      "Iteration: 7 - Loss: 0.26462\n",
      "Iteration: 8 - Loss: 0.23031\n",
      "Iteration: 9 - Loss: 0.22586\n",
      "Iteration: 10 - Loss: 0.23271\n",
      "Iteration: 11 - Loss: 0.24115\n",
      "Iteration: 12 - Loss: 0.22540\n",
      "Iteration: 13 - Loss: 0.26987\n",
      "Iteration: 14 - Loss: 0.25201\n",
      "Iteration: 15 - Loss: 0.24279\n",
      "Iteration: 16 - Loss: 0.22260\n",
      "Iteration: 17 - Loss: 0.22462\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "model, loss = fit_raw_data(learn_set, \n",
    "            nb_train,\n",
    "            learning_rate=learning_rate,\n",
    "            # batch_size=batch_size,  # gamma=gamma,\n",
    "            num_epochs=num_epochs,\n",
    "            betas=betas,\n",
    "            verbose=True,\n",
    "        )\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = tonic.datasets.NMNIST(save_to='../Data/',\n",
    "                                train=False, download=download,\n",
    "                                transform=tonic.transforms.AERtoVector())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_test = int(len(test_set)//ds)\n",
    "print(f'The dataset has size {len(test_set)}, using {nb_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(test_set, model, # gamma=gamma,\n",
    "            verbose=False, **kwargs\n",
    "        ):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "        sampler = torch.utils.data.RandomSampler(test_set, replacement=True, num_samples=nb_test, generator=generator)\n",
    "        loader = tonic.datasets.DataLoader(test_set, sampler=sampler)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        logistic_model = model.to(device)\n",
    "\n",
    "        pred_target, true_target = [], []\n",
    "\n",
    "        for X, label in loader:\n",
    "            X = X.to(device)\n",
    "            X, label = X.squeeze(0), label.squeeze(0)\n",
    "\n",
    "            n_events = X.shape[0]\n",
    "            labels = label*torch.ones(n_events).type(torch.LongTensor)\n",
    "\n",
    "            outputs = logistic_model(X)\n",
    "\n",
    "            pred_target.append(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            true_target.append(labels.numpy())\n",
    "\n",
    "    return pred_target, true_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_target, true_target = predict_data(test_set, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "for pred_target_, true_target_ in zip(pred_target, true_target):\n",
    "    accuracy.append(np.mean(pred_target_ == true_target_))\n",
    "print(f'{np.mean(accuracy)=:.3f}')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
