{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP 04 - Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joe/Documents/boulot/git/homhots/HOTS/HOTS\n"
     ]
    }
   ],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "%matplotlib inline\n",
    "%cd '../HOTS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "from mix_Network import *\n",
    "\n",
    "dataset = 'nmnist'\n",
    "records_path = '../Records'\n",
    "timestr = datetime.datetime.now().date().isoformat()\n",
    "timestr = '2020-12-21'\n",
    "verbose = True\n",
    "\n",
    "homeo = True\n",
    "sigma = None\n",
    "pooling = False\n",
    "homeinv = False\n",
    "jitter = False\n",
    "tau = 5\n",
    "nb_train = 500\n",
    "nb_test = 100\n",
    "krnlinit = 'first'\n",
    "nblay = 3\n",
    "nbclust = 4\n",
    "\n",
    "ds = 75\n",
    "nb_train = 7500//ds\n",
    "nb_test = 2500//ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nmnist(NbTrainingData, NbTestingData):\n",
    "    def loadev(nbdigit, loader, learningset):\n",
    "        timout = []\n",
    "        xout = []\n",
    "        yout = []\n",
    "        polout = []\n",
    "        labout = []\n",
    "        for i in range(NbTrainingData):\n",
    "            events, target = next(iter(loader))\n",
    "            for iev in range(events[0].shape[1]):\n",
    "                    timout.append(events[0][iev][learningset.ordering.find(\"t\")].item())\n",
    "                    xout.append(events[0][iev][learningset.ordering.find(\"x\")].item())\n",
    "                    yout.append(events[0][iev][learningset.ordering.find(\"y\")].item())\n",
    "                    polout.append(events[0][iev][learningset.ordering.find(\"p\")].item())\n",
    "                    labout.append(target.item())\n",
    "\n",
    "        eventsout = [xout,yout,timout,polout,labout,learningset.sensor_size,2]\n",
    "        return eventsout\n",
    "    \n",
    "    learningset = tonic.datasets.NMNIST(save_to='../Data/',\n",
    "                                    train=False,\n",
    "                                    transform=None)\n",
    "    loader = tonic.datasets.DataLoader(learningset, shuffle=True)\n",
    "    \n",
    "    events_train = loadev(NbTrainingData, loader, learningset)\n",
    "    events_test = loadev(NbTestingData, loader, learningset)\n",
    "    \n",
    "    return events_train, events_test\n",
    "\n",
    "#### Building matrix for logistic regression\n",
    "def gather_data(events_in,\n",
    "                tau_cla=150, # characteristic time of a digit\n",
    "                sample_events=50, sample_space = 1,\n",
    "                verbose=False, debug=False):\n",
    "    tau_cla *= 1e3 # to enter tau in ms\n",
    "    n_events = len(events_in[0])\n",
    "\n",
    "    c_int = lambda n, d : ((n - 1) // d) + 1\n",
    "    \n",
    "    data = np.zeros((c_int(events_in[-2][0], sample_space),\n",
    "                     c_int(events_in[-2][1], sample_space),\n",
    "                     events_in[-1])) #tmp data\n",
    "\n",
    "    X = np.zeros((c_int(n_events, sample_events), len(data.ravel())))\n",
    "    y = np.zeros((c_int(n_events, sample_events), ))\n",
    "\n",
    "    for i_event in range(1, n_events):\n",
    "        if events_in[2][i_event]<events_in[2][i_event-1]:\n",
    "            data = np.zeros((c_int(events_in[-2][0], sample_space),\n",
    "                     c_int(events_in[-2][1], sample_space),\n",
    "                     events_in[-1])) #tmp data \n",
    "\n",
    "        data *= np.exp(-(events_in[2][i_event]-events_in[2][i_event-1])/tau_cla)\n",
    "        \n",
    "        x_pos = events_in[0][i_event]//sample_space\n",
    "        y_pos = events_in[1][i_event]//sample_space\n",
    "        p = events_in[3][i_event]\n",
    "        data[int(x_pos), int(y_pos), int(p)] = 1.\n",
    "\n",
    "        if i_event % sample_events == sample_events//2 :\n",
    "            if debug:\n",
    "                print(f'DEBUG {i_event} {i_event//sample_events} ')\n",
    "                print(f'DEBUG {y[i_event//sample_events]}   ')\n",
    "                print(f'DEBUG  {events_in[-3][i_event]} ')\n",
    "            X[i_event//sample_events, :] = data.ravel()\n",
    "            y[i_event//sample_events] = events_in[-3][i_event]\n",
    "            \n",
    "    if verbose: print('Number of events: ' + str(X.shape[0])+' - Number of features: ' + str(X.shape[1]))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def get_events(tau, krnlinit, nblay, nbclust, homeo, records_path, timestr, dataset, sigma, homeinv, jitter, nb_train, nb_test):\n",
    "    hotshom = network(krnlinit=krnlinit, tau=tau, nblay=nblay, nbclust=nbclust, homeo=homeo, sigma=sigma, homeinv=homeinv, jitter=jitter)\n",
    "    arch = [hotshom.L[i].kernel.shape[1] for i in range(len(hotshom.L))]\n",
    "    fname = f'{records_path}/EXP_03_NMNIST/{timestr}_hots_{dataset}_{arch}_{tau}_{homeo}_{sigma}_{homeinv}_{jitter}'\n",
    "    print(fname)\n",
    "    if not os.path.isfile(fname+'_model.pkl'):\n",
    "        loaderhom, order = hotshom.learning1by1(dataset=dataset)\n",
    "        with open(fname+'_model.pkl', 'wb') as file:\n",
    "            pickle.dump([hotshom, loaderhom, order], file, pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(fname+'_model.pkl', 'rb') as file:\n",
    "            hotshom, loaderhom, order = pickle.load(file)\n",
    "            \n",
    "    if not os.path.isfile(fname+f'_evout_{nb_train}_{nb_test}.pkl'):       \n",
    "        _, loaderhom, out_train = hotshom.running(loaderhom, order, nb_digit = nb_train, LR=True)\n",
    "        _, loaderhom, out_test = hotshom.running(loaderhom, order, nb_digit = nb_test, LR=True)\n",
    "        with open(fname+f'_evout_{nb_train}_{nb_test}.pkl', 'wb') as file:\n",
    "            pickle.dump([out_train, out_test], file, pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(fname+f'_evout_{nb_train}_{nb_test}.pkl', 'rb') as file:\n",
    "            out_train, out_test = pickle.load(file)\n",
    "    return out_train, out_test\n",
    "\n",
    "\n",
    "# ### Performingxlogistic regression\n",
    "#\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html\n",
    "#\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.linear_model import LogisticRegressionCV as LR\n",
    "# \n",
    "opts_LR = dict(max_iter=2000, # random_state=0,\n",
    "               n_jobs=-1, class_weight='balanced')\n",
    "#opts_LR['Cs'] = 5\n",
    "opts_LR['Cs'] = 32\n",
    "# TODO for a publication use 100 from 10^{-10} to 10\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def tic():\n",
    "    global ttic\n",
    "    ttic = time.time()\n",
    "def toc():\n",
    "    print(f'Done in {time.time() - ttic:.3f} s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing logistic regression on raw input spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic()\n",
    "events_train, events_test = get_nmnist(nb_train, nb_test)\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic()\n",
    "X_train, y_train = gather_data(events_train, verbose=verbose)\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic()\n",
    "lr = LR(**opts_LR).fit(X_train, y_train)\n",
    "print(f'Classification score for raw input is {lr.score(X_train, y_train):.3f} (train)')\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic()\n",
    "X_test, y_test = gather_data(events_test, verbose=verbose)\n",
    "print(f'Classification score for raw input is {lr.score(X_test, y_test):.3f} (test)')\n",
    "toc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing logistic regression with or without homeostasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "homeo=False\n",
      "----------------------------------------\n",
      "../Records/EXP_03_NMNIST/2020-12-21_hots_nmnist_[4, 8, 16]_5_False_None_False_False\n",
      "Number of events: 7924 - Number of features: 18496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 57.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification score for homeo = False is 1.000 (train)\n",
      "Number of events: 2697 - Number of features: 18496\n",
      "Classification score for homeo = False is 0.608 (test)\n",
      "----------------------------------------\n",
      "homeo=True\n",
      "----------------------------------------\n",
      "../Records/EXP_03_NMNIST/2020-12-21_hots_nmnist_[4, 8, 16]_5_True_None_False_False\n",
      "Using downloaded and verified file: ../Data/nmnist_test.zip\n",
      "Extracting ../Data/nmnist_test.zip to ../Data/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55e7bd606884329a63ca70e8f2ec928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f32078dd8d4c27a5136056bbe33414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a5a8c1675c4d0aaa60f4af9d4a47ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8696a25e9742f388e1c08090bd50df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b98b2636a84bc8b6d47751fa4bdb1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=33.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of events: 7997 - Number of features: 18496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "for homeo in [False, True]:\n",
    "    print(40*'-')\n",
    "    print(f'homeo={homeo}')\n",
    "    print(40*'-')\n",
    "    events_train_o, events_test_o = get_events(tau, krnlinit, nblay, nbclust, homeo, records_path, timestr, dataset, sigma, homeinv, jitter, nb_train, nb_test)\n",
    "\n",
    "    X_train, y_train = gather_data(events_train_o, verbose=verbose)\n",
    "\n",
    "    lr = LR(**opts_LR, verbose=verbose).fit(X_train, y_train)\n",
    "    print(f'Classification score for homeo = {homeo} is {lr.score(X_train, y_train):.3f} (train)')\n",
    "\n",
    "    X_test, y_test = gather_data(events_test_o, verbose=verbose)\n",
    "    print(f'Classification score for homeo = {homeo} is {lr.score(X_test, y_test):.3f} (test)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
