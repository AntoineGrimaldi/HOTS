{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modèle génératif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 256 # number of factors\n",
    "n_classes = 10 # number of classes\n",
    "N_batch = 4\n",
    "seed = 1973\n",
    "\n",
    "np.random.seed(seed)\n",
    "W = np.random.randn(N+1, n_classes) # FIXED design matrix\n",
    "\n",
    "def psychometric_function(W, factors):\n",
    "    print(W.shape, factors.shape)\n",
    "    logit = (factors @ W[:-1, :]) + W[-1, :]\n",
    "    return 1 / (1 + np.exp(-logit))\n",
    "\n",
    "def get_data(W, seed, N_batch):\n",
    "    N, n_classes = W.shape[0]-1, W.shape[1]\n",
    "    np.random.seed(seed)\n",
    "    factors = np.random.randn(N_batch, N)\n",
    "    p = psychometric_function(W, factors)\n",
    "    y = p > np.random.rand(N_batch, n_classes)  # generate data\n",
    "    \n",
    "    return factors, p, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = np.random.randn(N_batch, N)\n",
    "factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(factors@W[:-1, :]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((factors@W[:-1, :]) + W[-1, :]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psychometric_function(W, factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors, p, y = get_data(W, seed, N_batch)\n",
    "factors.shape, p.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(p.ravel(), bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "torch.set_default_tensor_type(\"torch.DoubleTensor\") # -> torch.tensor([1.2, 3]).dtype = torch.float64\n",
    "criterion = torch.nn.BCELoss(reduction=\"mean\") # loss divided by output size\n",
    "#criterion = torch.nn.NLLLoss(reduction=\"mean\") # loss divided by output size\n",
    "\n",
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    #torch.nn.Module -> Base class for all neural network modules\n",
    "    def __init__(self, N, n_classes, bias=True):\n",
    "        super(LogisticRegressionModel, self).__init__() \n",
    "        self.linear = torch.nn.Linear(N, n_classes, bias=bias)\n",
    "        # self.nl = torch.nn.LogSoftmax(n_classes)\n",
    "        self.nl = torch.nn.Sigmoid(n_classes)\n",
    "\n",
    "    def forward(self, factors):\n",
    "        return self.nl(self.linear(factors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "betas = (beta1, beta2)\n",
    "num_epochs = 2 ** 9 + 1\n",
    "batch_size = 256\n",
    "n_classes=10\n",
    "amsgrad = False # gives similar results\n",
    "amsgrad = True  # gives similar results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegressionModel(N, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = torch.randn(N_batch, N)\n",
    "outputs = logistic_model(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_data(factors, y, \n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,  # gamma=gamma,\n",
    "            num_epochs=num_epochs,\n",
    "            betas=betas,\n",
    "            verbose=False, **kwargs\n",
    "        ):\n",
    "\n",
    "    X, labels = torch.Tensor(factors[:, None]), torch.Tensor(y[:, None])\n",
    "    loader = DataLoader(\n",
    "        TensorDataset(X, labels), batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    N_batch = factors.shape[0]\n",
    "    N = factors.shape[1]\n",
    "    n_classes = y.shape[1]\n",
    "    logistic_model = LogisticRegressionModel(N, n_classes)\n",
    "    logistic_model = logistic_model.to(device)\n",
    "    logistic_model.train()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        logistic_model.parameters(), lr=learning_rate, betas=betas, amsgrad=amsgrad\n",
    "    )\n",
    "    for epoch in range(int(num_epochs)):\n",
    "        logistic_model.train()\n",
    "        losses = []\n",
    "        for X_, labels_ in loader:\n",
    "            X_, labels_ = X_.to(device), labels_.to(device)\n",
    "            outputs = logistic_model(X_)\n",
    "            loss = criterion(outputs, labels_)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        if verbose and (epoch % (num_epochs // 32) == 0):\n",
    "            print(f\"Iteration: {epoch} - Loss: {np.mean(losses):.5f}\")\n",
    "\n",
    "    logistic_model.eval()\n",
    "    X, labels = torch.Tensor(factors[:, None]), torch.Tensor(y[:, None])\n",
    "    outputs = logistic_model(X)\n",
    "    loss = criterion(outputs, labels).item()\n",
    "    return logistic_model, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors, p, y = get_data(W, seed=seed, N_batch=10000)\n",
    "logistic_model, loss = fit_data(factors, y, verbose=True)\n",
    "print(\"Final loss =\", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
